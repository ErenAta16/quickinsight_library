"""
QuickInsights yardÄ±mcÄ± fonksiyonlar modÃ¼lÃ¼

Bu modÃ¼l, veri analizi iÃ§in gerekli yardÄ±mcÄ± fonksiyonlarÄ± iÃ§erir.
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, Tuple
import os


def get_data_info(df: pd.DataFrame) -> Dict[str, Any]:
    """
    Veri seti hakkÄ±nda genel bilgileri dÃ¶ndÃ¼rÃ¼r.
    
    Parameters
    ----------
    df : pd.DataFrame
        Analiz edilecek veri seti
        
    Returns
    -------
    dict
        Veri seti bilgilerini iÃ§eren sÃ¶zlÃ¼k
    """
    
    info = {
        'rows': len(df),
        'columns': len(df.columns),
        'memory_usage': df.memory_usage(deep=True).sum() / 1024 / 1024,  # MB cinsinden
        'dtypes': df.dtypes.value_counts().to_dict(),
        'missing_values': df.isnull().sum().sum(),
        'duplicate_rows': df.duplicated().sum(),
        'numeric_columns': len(df.select_dtypes(include=[np.number]).columns),
        'categorical_columns': len(df.select_dtypes(include=['object', 'category']).columns),
        'datetime_columns': len(df.select_dtypes(include=['datetime64']).columns)
    }
    
    return info


def detect_outliers(df: pd.DataFrame, method: str = 'iqr', threshold: float = 1.5) -> pd.DataFrame:
    """
    SayÄ±sal deÄŸiÅŸkenlerde aykÄ±rÄ± deÄŸerleri tespit eder.
    
    Parameters
    ----------
    df : pd.DataFrame
        Sadece sayÄ±sal deÄŸiÅŸkenler iÃ§eren veri seti
    method : str, default='iqr'
        AykÄ±rÄ± deÄŸer tespit yÃ¶ntemi ('iqr' veya 'zscore')
    threshold : float, default=1.5
        AykÄ±rÄ± deÄŸer eÅŸiÄŸi
        
    Returns
    -------
    pd.DataFrame
        Boolean mask - True deÄŸerler aykÄ±rÄ± deÄŸerleri gÃ¶sterir
    """
    
    if df.empty:
        return pd.DataFrame()
    
    # Vectorized operations iÃ§in numpy array'e Ã§evir
    data = df.values
    outliers = np.zeros_like(data, dtype=bool)
    
    if method == 'iqr':
        # IQR yÃ¶ntemi - vectorized
        Q1 = np.percentile(data, 25, axis=0)
        Q3 = np.percentile(data, 75, axis=0)
        IQR = Q3 - Q1
        lower_bound = Q1 - threshold * IQR
        upper_bound = Q3 + threshold * IQR
        
        # Broadcasting ile tÃ¼m kolonlarÄ± aynÄ± anda iÅŸle
        outliers = (data < lower_bound) | (data > upper_bound)
        
    elif method == 'zscore':
        # Z-score yÃ¶ntemi - vectorized
        mean_vals = np.mean(data, axis=0)
        std_vals = np.std(data, axis=0)
        
        # Broadcasting ile tÃ¼m kolonlarÄ± aynÄ± anda iÅŸle
        z_scores = np.abs((data - mean_vals) / std_vals)
        outliers = z_scores > threshold
    
    else:
        raise ValueError("GeÃ§ersiz yÃ¶ntem! 'iqr' veya 'zscore' kullanÄ±n.")
    
    return pd.DataFrame(outliers, index=df.index, columns=df.columns)


def get_correlation_strength(correlation: float) -> str:
    """
    Korelasyon katsayÄ±sÄ±nÄ±n gÃ¼cÃ¼nÃ¼ sÄ±nÄ±flandÄ±rÄ±r.
    
    Parameters
    ----------
    correlation : float
        Korelasyon katsayÄ±sÄ± (-1 ile 1 arasÄ±)
        
    Returns
    -------
    str
        Korelasyon gÃ¼cÃ¼ aÃ§Ä±klamasÄ±
    """
    
    abs_corr = abs(correlation)
    
    if abs_corr >= 0.8:
        return "Ã‡ok gÃ¼Ã§lÃ¼"
    elif abs_corr >= 0.6:
        return "GÃ¼Ã§lÃ¼"
    elif abs_corr >= 0.4:
        return "Orta"
    elif abs_corr >= 0.2:
        return "ZayÄ±f"
    else:
        return "Ã‡ok zayÄ±f"


def format_number(value: float, decimals: int = 4) -> str:
    """
    SayÄ±larÄ± okunabilir formatta dÃ¶ndÃ¼rÃ¼r.
    
    Parameters
    ----------
    value : float
        Formatlanacak sayÄ±
    decimals : int, default=4
        OndalÄ±k basamak sayÄ±sÄ±
        
    Returns
    -------
    str
        FormatlanmÄ±ÅŸ sayÄ±
    """
    
    if pd.isna(value):
        return "N/A"
    
    if abs(value) >= 1000000:
        return f"{value/1000000:.{decimals}f}M"
    elif abs(value) >= 1000:
        return f"{value/1000:.{decimals}f}K"
    else:
        return f"{value:.{decimals}f}"


def create_output_directory(output_dir: str) -> str:
    """
    Ã‡Ä±ktÄ± dizinini oluÅŸturur.
    
    Parameters
    ----------
    output_dir : str
        OluÅŸturulacak dizin yolu
        
    Returns
    -------
    str
        OluÅŸturulan dizin yolu
    """
    
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"ğŸ“ Ã‡Ä±ktÄ± dizini oluÅŸturuldu: {output_dir}")
    
    return output_dir


def validate_dataframe(df: pd.DataFrame) -> bool:
    """
    Veri setinin geÃ§erli olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.
    
    Parameters
    ----------
    df : pd.DataFrame
        Kontrol edilecek veri seti
        
    Returns
    -------
    bool
        Veri seti geÃ§erliyse True
    """
    
    if not isinstance(df, pd.DataFrame):
        raise TypeError("Veri seti pandas DataFrame olmalÄ±dÄ±r!")
    
    if df.empty:
        raise ValueError("Veri seti boÅŸ olamaz!")
    
    if len(df.columns) == 0:
        raise ValueError("Veri setinde sÃ¼tun bulunamadÄ±!")
    
    return True


def get_data_sample(df: pd.DataFrame, sample_size: int = 5) -> pd.DataFrame:
    """
    Veri setinden Ã¶rnek satÄ±rlar dÃ¶ndÃ¼rÃ¼r.
    
    Parameters
    ----------
    df : pd.DataFrame
        Ã–rnek alÄ±nacak veri seti
    sample_size : int, default=5
        Ã–rnek satÄ±r sayÄ±sÄ±
        
    Returns
    -------
    pd.DataFrame
        Ã–rnek veri seti
    """
    
    if len(df) <= sample_size:
        return df
    
    # Vectorized sampling - numpy array kullanarak hÄ±zlandÄ±r
    total_rows = len(df)
    
    # Ä°lk, orta ve son satÄ±rlardan Ã¶rnek al - daha verimli
    indices = []
    
    # Ä°lk birkaÃ§ satÄ±r
    first_count = min(sample_size // 2, total_rows)
    indices.extend(range(first_count))
    
    # Ortadan birkaÃ§ satÄ±r
    mid_count = max(1, sample_size // 4)
    mid_start = total_rows // 2 - mid_count // 2
    mid_end = total_rows // 2 + mid_count // 2
    indices.extend(range(mid_start, min(mid_end, total_rows)))
    
    # Son birkaÃ§ satÄ±r
    last_count = max(1, sample_size - len(indices))
    indices.extend(range(max(0, total_rows - last_count), total_rows))
    
    # Benzersiz indeksleri al ve sÄ±rala - set kullanarak hÄ±zlandÄ±r
    unique_indices = sorted(set(indices))
    
    # NumPy array slicing ile daha hÄ±zlÄ±
    return df.iloc[list(unique_indices)]


def optimize_dtypes(df: pd.DataFrame) -> pd.DataFrame:
    """
    Veri tiplerini optimize eder ve bellek kullanÄ±mÄ±nÄ± azaltÄ±r.
    
    Parameters
    ----------
    df : pd.DataFrame
        Optimize edilecek veri seti
        
    Returns
    -------
    pd.DataFrame
        Optimize edilmiÅŸ veri seti
    """
    print("ğŸ”§ Veri tipleri optimize ediliyor...")
    
    initial_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
    
    # Vectorized dtype optimization - tÃ¼m kolonlarÄ± aynÄ± anda iÅŸle
    
    # Float64'leri float32'ye dÃ¼ÅŸÃ¼r
    float64_cols = df.select_dtypes(include=['float64']).columns
    if len(float64_cols) > 0:
        df[float64_cols] = df[float64_cols].astype('float32')
        print(f"   ğŸ”§ {len(float64_cols)} float64 kolonu float32'ye dÃ¼ÅŸÃ¼rÃ¼ldÃ¼")
    
    # Int64'leri int32'ye dÃ¼ÅŸÃ¼r (mÃ¼mkÃ¼nse)
    int64_cols = df.select_dtypes(include=['int64']).columns
    if len(int64_cols) > 0:
        # TÃ¼m int64 kolonlarÄ± aynÄ± anda kontrol et
        for col in int64_cols:
            col_min = df[col].min()
            col_max = df[col].max()
            if col_min >= -2147483648 and col_max <= 2147483647:
                df[col] = df[col].astype('int32')
        print(f"   ğŸ”§ {len(int64_cols)} int64 kolonu optimize edildi")
    
    # Object tiplerini kategorik yap (mÃ¼mkÃ¼nse)
    object_cols = df.select_dtypes(include=['object']).columns
    if len(object_cols) > 0:
        # TÃ¼m object kolonlarÄ± aynÄ± anda kontrol et
        for col in object_cols:
            if df[col].nunique() / len(df) < 0.5:  # %50'den az benzersiz deÄŸer
                df[col] = df[col].astype('category')
        print(f"   ğŸ”§ {len(object_cols)} object kolonu optimize edildi")
    
    final_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
    memory_saved = initial_memory - final_memory
    
    print(f"   ğŸ’¾ Bellek tasarrufu: {memory_saved:.2f} MB ({memory_saved/initial_memory*100:.1f}%)")
    
    return df


# Cache sistemi iÃ§in gerekli importlar
import json
import time

# Numba JIT compilation iÃ§in
try:
    from numba import jit, prange
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False
    print("âš ï¸  Numba bulunamadÄ±. JIT compilation devre dÄ±ÅŸÄ±.")

# Dask entegrasyonu iÃ§in
try:
    import dask.dataframe as dd
    import dask.array as da
    from dask.distributed import Client, LocalCluster
    DASK_AVAILABLE = True
except ImportError:
    DASK_AVAILABLE = False
    print("âš ï¸  Dask bulunamadÄ±. BÃ¼yÃ¼k veri analizi devre dÄ±ÅŸÄ±.")

# Memory mapping iÃ§in
try:
    import mmap
    import tempfile
    import os
    MEMORY_MAPPING_AVAILABLE = True
except ImportError:
    MEMORY_MAPPING_AVAILABLE = False
    print("âš ï¸  Memory mapping bulunamadÄ±.")

# Profiling araÃ§larÄ± iÃ§in
try:
    import cProfile
    import pstats
    import io
    import psutil
    import tracemalloc
    PROFILING_AVAILABLE = True
except ImportError:
    PROFILING_AVAILABLE = False
    print("âš ï¸  Profiling araÃ§larÄ± bulunamadÄ±.")

# Async/await desteÄŸi iÃ§in
try:
    import asyncio
    import aiofiles
    import aiohttp
    ASYNC_AVAILABLE = True
except ImportError:
    ASYNC_AVAILABLE = False
    print("âš ï¸  Async/await desteÄŸi bulunamadÄ±.")

# GPU acceleration iÃ§in
try:
    import cupy as cp
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False
    print("âš ï¸  GPU acceleration bulunamadÄ±.")

# Cloud deployment iÃ§in
try:
    import boto3
    import azure.storage.blob
    import google.cloud.storage
    CLOUD_AVAILABLE = True
except ImportError:
    CLOUD_AVAILABLE = False
    print("âš ï¸  Cloud deployment bulunamadÄ±.")


class AnalysisCache:
    """
    Analiz sonuÃ§larÄ±nÄ± cache'leyen sÄ±nÄ±f.
    
    Bu sÄ±nÄ±f, veri seti hash'lerine gÃ¶re analiz sonuÃ§larÄ±nÄ± saklar
    ve tekrar analizleri Ã¶nler.
    """
    
    def __init__(self, cache_dir: str = "./.quickinsights_cache", max_size: int = 100):
        """
        AnalysisCache'i baÅŸlatÄ±r.
        
        Parameters
        ----------
        cache_dir : str
            Cache dosyalarÄ±nÄ±n saklanacaÄŸÄ± dizin
        max_size : int
            Maksimum cache boyutu (MB)
        """
        self.cache_dir = cache_dir
        self.max_size = max_size
        self.cache_info = {}
        
        # Cache dizinini oluÅŸtur
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir)
            print(f"ğŸ“ Cache dizini oluÅŸturuldu: {cache_dir}")
        
        # Mevcut cache'leri yÃ¼kle
        self._load_cache_info()
    
    def _load_cache_info(self):
        """Mevcut cache bilgilerini yÃ¼kler"""
        cache_info_file = os.path.join(self.cache_dir, "cache_info.json")
        if os.path.exists(cache_info_file):
            try:
                with open(cache_info_file, 'r', encoding='utf-8') as f:
                    self.cache_info = json.load(f)
            except:
                self.cache_info = {}
    
    def _save_cache_info(self):
        """Cache bilgilerini kaydeder"""
        cache_info_file = os.path.join(self.cache_dir, "cache_info.json")
        try:
            with open(cache_info_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_info, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  Cache bilgileri kaydedilemedi: {e}")
    
    def _get_dataframe_hash(self, df: pd.DataFrame) -> str:
        """
        DataFrame iÃ§in benzersiz hash oluÅŸturur.
        
        Parameters
        ----------
        df : pd.DataFrame
            Hash'i oluÅŸturulacak DataFrame
            
        Returns
        -------
        str
            DataFrame'in hash'i
        """
        # DataFrame'in iÃ§eriÄŸini string'e Ã§evir
        df_str = df.to_string()
        
        # Hash oluÅŸtur
        import hashlib
        return hashlib.md5(df_str.encode()).hexdigest()
    
    def get_cached_result(self, df: pd.DataFrame, analysis_type: str):
        """
        Cache'den sonuÃ§ alÄ±r.
        
        Parameters
        ----------
        df : pd.DataFrame
            Analiz edilecek DataFrame
        analysis_type : str
            Analiz tÃ¼rÃ¼
            
        Returns
        -------
        Any or None
            Cache'den alÄ±nan sonuÃ§ veya None
        """
        df_hash = self._get_dataframe_hash(df)
        cache_key = f"{df_hash}_{analysis_type}"
        
        if cache_key in self.cache_info:
            cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
            if os.path.exists(cache_file):
                try:
                    import pickle
                    with open(cache_file, 'rb') as f:
                        result = pickle.load(f)
                    
                    print(f"ğŸ’¾ Cache'den alÄ±ndÄ±: {analysis_type}")
                    return result
                except Exception as e:
                    print(f"âš ï¸  Cache okuma hatasÄ±: {e}")
                    # HatalÄ± cache'i sil
                    self._remove_cache_entry(cache_key)
        
        return None
    
    def cache_result(self, df: pd.DataFrame, analysis_type: str, result) -> bool:
        """
        Sonucu cache'ler.
        
        Parameters
        ----------
        df : pd.DataFrame
            Analiz edilen DataFrame
        analysis_type : str
            Analiz tÃ¼rÃ¼
        result : Any
            Cache'lenecek sonuÃ§
            
        Returns
        -------
        bool
            Cache baÅŸarÄ±lÄ± ise True
        """
        try:
            df_hash = self._get_dataframe_hash(df)
            cache_key = f"{df_hash}_{analysis_type}"
            
            # Cache dosyasÄ± oluÅŸtur
            cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
            
            import pickle
            with open(cache_file, 'wb') as f:
                pickle.dump(result, f)
            
            # Cache bilgilerini gÃ¼ncelle
            file_size = os.path.getsize(cache_file) / 1024 / 1024  # MB
            self.cache_info[cache_key] = {
                'analysis_type': analysis_type,
                'df_hash': df_hash,
                'file_size': file_size,
                'created_at': time.time()
            }
            
            # Cache boyutunu kontrol et
            self._manage_cache_size()
            
            return True
        except Exception as e:
            print(f"âš ï¸  Cache hatasÄ±: {e}")
            return False
    
    def get_or_compute(self, cache_key: str, compute_func, *args, **kwargs):
        """
        Cache'den sonucu alÄ±r veya hesaplar.
        
        Parameters
        ----------
        cache_key : str
            Cache anahtarÄ±
        compute_func : callable
            Hesaplama fonksiyonu
        *args, **kwargs
            Hesaplama fonksiyonuna geÃ§irilecek argÃ¼manlar
            
        Returns
        -------
        Any
            Cache'den alÄ±nan veya hesaplanan sonuÃ§
        """
        # Ã–nce cache'den kontrol et
        if cache_key in self.cache_info:
            cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
            if os.path.exists(cache_file):
                try:
                    import pickle
                    with open(cache_file, 'rb') as f:
                        result = pickle.load(f)
                    print(f"ğŸ’¾ Cache'den alÄ±ndÄ±: {cache_key}")
                    return result
                except Exception as e:
                    print(f"âš ï¸  Cache okuma hatasÄ±: {e}")
                    # HatalÄ± cache'i sil
                    self._remove_cache_entry(cache_key)
        
        # Cache'de yoksa hesapla
        print(f"ğŸ§® HesaplanÄ±yor: {cache_key}")
        result = compute_func(*args, **kwargs)
        
        # Sonucu cache'le
        try:
            cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
            import pickle
            with open(cache_file, 'wb') as f:
                pickle.dump(result, f)
            
            # Cache bilgilerini gÃ¼ncelle
            file_size = os.path.getsize(cache_file) / 1024 / 1024  # MB
            self.cache_info[cache_key] = {
                'analysis_type': cache_key,
                'df_hash': cache_key,
                'file_size': file_size,
                'created_at': time.time()
            }
            
            # Cache boyutunu kontrol et
            self._manage_cache_size()
            
        except Exception as e:
            print(f"âš ï¸  Cache kaydetme hatasÄ±: {e}")
        
        return result
    
    def _manage_cache_size(self):
        """Cache boyutunu yÃ¶netir"""
        total_size = sum(info['file_size'] for info in self.cache_info.values())
        
        if total_size > self.max_size:
            print(f"âš ï¸  Cache boyutu aÅŸÄ±ldÄ± ({total_size:.2f} MB > {self.max_size} MB)")
            print("ğŸ—‘ï¸  Eski cache'ler temizleniyor...")
            
            # En eski cache'leri sil
            sorted_cache = sorted(self.cache_info.items(), 
                                key=lambda x: x[1]['created_at'])
            
            for i, (key, info) in enumerate(sorted_cache):
                if total_size <= self.max_size:
                    break
                
                cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
                if os.path.exists(cache_file):
                    os.remove(cache_file)
                    total_size -= info['file_size']
                    del self.cache_info[key]
                    print(f"   ğŸ—‘ï¸  Silindi: {info['analysis_type']}")
    
    def _remove_cache_entry(self, cache_key: str):
        """Cache giriÅŸini kaldÄ±rÄ±r"""
        if cache_key in self.cache_info:
            cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
            if os.path.exists(cache_file):
                os.remove(cache_file)
            del self.cache_info[cache_key]
    
    def clear_cache(self):
        """TÃ¼m cache'i temizler"""
        for cache_key in list(self.cache_info.keys()):
            self._remove_cache_entry(cache_key)
        
        # Cache bilgilerini kaydet
        self._save_cache_info()
        print("ğŸ—‘ï¸  TÃ¼m cache temizlendi!")
    
    def get_cache_stats(self):
        """Cache istatistiklerini dÃ¶ndÃ¼rÃ¼r"""
        total_files = len(self.cache_info)
        total_size = sum(info['file_size'] for info in self.cache_info.values())
        
        print("ğŸ“Š Cache Ä°statistikleri:")
        print(f"   ğŸ“ Toplam cache dosyasÄ±: {total_files}")
        print(f"   ğŸ’¾ Toplam boyut: {total_size:.2f} MB")
        print(f"   ğŸ¯ Maksimum boyut: {self.max_size} MB")
        
        # Analiz tÃ¼rlerine gÃ¶re daÄŸÄ±lÄ±m
        analysis_counts = {}
        for info in self.cache_info.values():
            analysis_type = info['analysis_type']
            analysis_counts[analysis_type] = analysis_counts.get(analysis_type, 0) + 1
        
        print(f"   ğŸ” Analiz tÃ¼rleri: {analysis_counts}")
        
        return {
            'total_files': total_files,
            'total_size': total_size,
            'max_size': self.max_size,
            'analysis_counts': analysis_counts
        }


# =============================================================================
# Numba JIT ile HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Fonksiyonlar
# =============================================================================

def fast_correlation_matrix(data):
    """
    Numba JIT ile hÄ±zlandÄ±rÄ±lmÄ±ÅŸ korelasyon matrisi hesaplama.
    
    Parameters
    ----------
    data : pandas.DataFrame or numpy.ndarray
        2D veri (satÄ±rlar, sÃ¼tunlar)
        
    Returns
    -------
    numpy.ndarray
        Korelasyon matrisi
    """
    # DataFrame'i numpy array'e Ã§evir
    if hasattr(data, 'values'):
        data = data.values
    
    # Numba JIT ile hesapla
    return _fast_correlation_matrix_numba(data)

@jit(nopython=True, parallel=True)
def _fast_correlation_matrix_numba(data):
    """
    Numba JIT ile hÄ±zlandÄ±rÄ±lmÄ±ÅŸ korelasyon matrisi hesaplama (numpy array iÃ§in).
    """
    n_rows, n_cols = data.shape
    corr_matrix = np.zeros((n_cols, n_cols))
    
    for i in prange(n_cols):
        for j in range(i, n_cols):
            if i == j:
                corr_matrix[i, j] = 1.0
                continue
            
            # Korelasyon hesaplama
            x = data[:, i]
            y = data[:, j]
            
            # Ortalama hesapla
            x_mean = np.mean(x)
            y_mean = np.mean(y)
            
            # Pay ve payda hesapla
            numerator = 0.0
            x_var = 0.0
            y_var = 0.0
            
            for k in range(n_rows):
                x_diff = x[k] - x_mean
                y_diff = y[k] - y_mean
                numerator += x_diff * y_diff
                x_var += x_diff * x_diff
                y_var += y_diff * y_diff
            
            # Korelasyon katsayÄ±sÄ±
            if x_var > 0 and y_var > 0:
                corr = numerator / np.sqrt(x_var * y_var)
                corr_matrix[i, j] = corr
                corr_matrix[j, i] = corr
    
    return corr_matrix


@jit(nopython=True, parallel=True)
def fast_outlier_detection(data, method='iqr', threshold=1.5):
    """
    Numba JIT ile hÄ±zlandÄ±rÄ±lmÄ±ÅŸ aykÄ±rÄ± deÄŸer tespiti.
    
    Parameters
    ----------
    data : numpy.ndarray
        2D numpy array (satÄ±rlar, sÃ¼tunlar)
    method : str
        'iqr' veya 'zscore'
    threshold : float
        AykÄ±rÄ± deÄŸer eÅŸiÄŸi
        
    Returns
    -------
    numpy.ndarray
        Boolean mask - True deÄŸerler aykÄ±rÄ± deÄŸerleri gÃ¶sterir
    """
    n_rows, n_cols = data.shape
    outliers = np.zeros((n_rows, n_cols), dtype=np.bool_)
    
    if method == 'iqr':
        for j in prange(n_cols):
            col = data[:, j]
            
            # Q1, Q3 ve IQR hesapla
            sorted_col = np.sort(col)
            q1_idx = int(0.25 * n_rows)
            q3_idx = int(0.75 * n_rows)
            
            q1 = sorted_col[q1_idx]
            q3 = sorted_col[q3_idx]
            iqr = q3 - q1
            
            lower_bound = q1 - threshold * iqr
            upper_bound = q3 + threshold * iqr
            
            for i in range(n_rows):
                outliers[i, j] = (col[i] < lower_bound) or (col[i] > upper_bound)
    
    elif method == 'zscore':
        for j in prange(n_cols):
            col = data[:, j]
            
            # Ortalama ve standart sapma
            mean_val = np.mean(col)
            std_val = np.std(col)
            
            if std_val > 0:
                for i in range(n_rows):
                    z_score = abs((col[i] - mean_val) / std_val)
                    outliers[i, j] = z_score > threshold
    
    return outliers


@jit(nopython=True, parallel=True)
def fast_summary_stats(data):
    """
    Numba JIT ile hÄ±zlandÄ±rÄ±lmÄ±ÅŸ istatistiksel Ã¶zet.
    
    Parameters
    ----------
    data : numpy.ndarray
        2D numpy array (satÄ±rlar, sÃ¼tunlar)
        
    Returns
    -------
    dict
        Her sÃ¼tun iÃ§in istatistikler
    """
    n_rows, n_cols = data.shape
    stats = {}
    
    for j in prange(n_cols):
        col = data[:, j]
        
        # Temel istatistikler
        mean_val = np.mean(col)
        std_val = np.std(col)
        min_val = np.min(col)
        max_val = np.max(col)
        
        # Medyan hesapla
        sorted_col = np.sort(col)
        if n_rows % 2 == 0:
            median_val = (sorted_col[n_rows//2 - 1] + sorted_col[n_rows//2]) / 2
        else:
            median_val = sorted_col[n_rows//2]
        
        # Q1 ve Q3 hesapla
        q1_idx = int(0.25 * n_rows)
        q3_idx = int(0.75 * n_rows)
        q1_val = sorted_col[q1_idx]
        q3_val = sorted_col[q3_idx]
        
        stats[j] = {
            'mean': mean_val,
            'median': median_val,
            'std': std_val,
            'min': min_val,
            'max': max_val,
            'q1': q1_val,
            'q3': q3_val
        }
    
    return stats


def get_numba_status():
    """Numba kullanÄ±labilirlik durumunu dÃ¶ndÃ¼rÃ¼r"""
    if NUMBA_AVAILABLE:
        print("âœ… Numba JIT compilation aktif!")
        print(f"   ğŸš€ HÄ±zlandÄ±rÄ±lmÄ±ÅŸ fonksiyonlar kullanÄ±labilir")
        return True
    else:
        print("âŒ Numba JIT compilation devre dÄ±ÅŸÄ±!")
        print("   ğŸ“¦ Kurulum: pip install numba")
        return False


def benchmark_numba_vs_pandas(df, n_iterations=100):
    """
    Numba vs Pandas performans karÅŸÄ±laÅŸtÄ±rmasÄ±.
    
    Parameters
    ----------
    df : pd.DataFrame
        Test edilecek veri seti
    n_iterations : int
        Test tekrar sayÄ±sÄ±
        
    Returns
    -------
    dict
        Benchmark sonuÃ§larÄ±
    """
    if not NUMBA_AVAILABLE:
        print("âŒ Numba bulunamadÄ±! Benchmark yapÄ±lamÄ±yor.")
        return {}
    
    print("ğŸ Numba vs Pandas Benchmark BaÅŸlÄ±yor...")
    print(f"   ğŸ“Š Veri seti boyutu: {df.shape}")
    print(f"   ğŸ”„ Test tekrar sayÄ±sÄ±: {n_iterations}")
    
    # SayÄ±sal sÃ¼tunlarÄ± al
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) == 0:
        print("âŒ SayÄ±sal sÃ¼tun bulunamadÄ±!")
        return {}
    
    # Veriyi numpy array'e Ã§evir
    data = df[numeric_cols].values
    
    # Pandas korelasyon testi
    print("\nğŸ” Pandas korelasyon testi...")
    start_time = time.time()
    for _ in range(n_iterations):
        pandas_corr = df[numeric_cols].corr().values
    pandas_time = time.time() - start_time
    
    # Numba korelasyon testi
    print("ğŸš€ Numba korelasyon testi...")
    start_time = time.time()
    for _ in range(n_iterations):
        numba_corr = fast_correlation_matrix(data)
    numba_time = time.time() - start_time
    
    # SonuÃ§larÄ± hesapla
    speedup = pandas_time / numba_time
    time_saved = ((pandas_time - numba_time) / pandas_time) * 100
    
    print(f"\nğŸ“ˆ BENCHMARK SONUÃ‡LARI:")
    print(f"   ğŸŒ Pandas: {pandas_time:.4f} saniye")
    print(f"   ğŸš€ Numba: {numba_time:.4f} saniye")
    print(f"   âš¡ HÄ±zlanma: {speedup:.2f}x")
    print(f"   ğŸ’° Zaman tasarrufu: {time_saved:.1f}%")
    
    return {
        'pandas_time': pandas_time,
        'numba_time': numba_time,
        'speedup': speedup,
        'time_saved': time_saved,
        'n_iterations': n_iterations
    }


# =============================================================================
# Memory Mapping ile Bellek Optimizasyonu
# =============================================================================

def get_memory_mapping_status():
    """Memory mapping kullanÄ±labilirlik durumunu dÃ¶ndÃ¼rÃ¼r"""
    if MEMORY_MAPPING_AVAILABLE:
        print("âœ… Memory mapping aktif!")
        print(f"   ğŸš€ BÃ¼yÃ¼k dosyalar iÃ§in bellek tasarrufu saÄŸlanabilir")
        return True
    else:
        print("âŒ Memory mapping devre dÄ±ÅŸÄ±!")
        return False


def create_memory_mapped_array(data, filename=None):
    """
    Memory mapped array oluÅŸturur.
    
    Parameters
    ----------
    data : numpy.ndarray
        Memory map'lenecek veri
    filename : str, optional
        GeÃ§ici dosya adÄ± (None ise otomatik)
        
    Returns
    -------
    numpy.memmap
        Memory mapped array
    """
    if not MEMORY_MAPPING_AVAILABLE:
        print("âŒ Memory mapping bulunamadÄ±!")
        return data
    
    try:
        if filename is None:
            # GeÃ§ici dosya oluÅŸtur
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mmap')
            filename = temp_file.name
            temp_file.close()
        
        # Memory mapped array oluÅŸtur
        mmap_array = np.memmap(filename, dtype=data.dtype, mode='w+', shape=data.shape)
        mmap_array[:] = data[:]
        
        print(f"âœ… Memory mapped array oluÅŸturuldu!")
        print(f"   ğŸ“ Dosya: {filename}")
        print(f"   ğŸ’¾ Boyut: {data.nbytes / 1024 / 1024:.2f} MB")
        
        return mmap_array
        
    except Exception as e:
        print(f"âŒ Memory mapping hatasÄ±: {e}")
        return data


def benchmark_memory_vs_mmap(data, n_iterations=100):
    """
    Normal array vs Memory mapped array performans karÅŸÄ±laÅŸtÄ±rmasÄ±.
    
    Parameters
    ----------
    data : numpy.ndarray
        Test edilecek veri
    n_iterations : int
        Test tekrar sayÄ±sÄ±
        
    Returns
    -------
    dict
        Benchmark sonuÃ§larÄ±
    """
    if not MEMORY_MAPPING_AVAILABLE:
        print("âŒ Memory mapping bulunamadÄ±! Benchmark yapÄ±lamÄ±yor.")
        return {}
    
    print("ğŸ Normal Array vs Memory Map Benchmark BaÅŸlÄ±yor...")
    print(f"   ğŸ“Š Veri boyutu: {data.shape}")
    print(f"   ğŸ’¾ Bellek kullanÄ±mÄ±: {data.nbytes / 1024 / 1024:.2f} MB")
    print(f"   ğŸ”„ Test tekrar sayÄ±sÄ±: {n_iterations}")
    
    # Normal array testi
    print("\nğŸ” Normal array testi...")
    start_time = time.time()
    for _ in range(n_iterations):
        # Basit operasyonlar
        mean_val = np.mean(data)
        std_val = np.std(data)
        min_val = np.min(data)
        max_val = np.max(data)
    normal_time = time.time() - start_time
    
    # Memory mapped array testi
    print("ğŸš€ Memory mapped array testi...")
    mmap_array = create_memory_mapped_array(data)
    if mmap_array is None:
        print("âŒ Memory mapped array oluÅŸturulamadÄ±!")
        return {}
    
    start_time = time.time()
    for _ in range(n_iterations):
        # AynÄ± operasyonlar
        mean_val = np.mean(mmap_array)
        std_val = np.std(mmap_array)
        min_val = np.min(mmap_array)
        max_val = np.max(mmap_array)
    mmap_time = time.time() - start_time
    
    # SonuÃ§larÄ± hesapla
    speedup = normal_time / mmap_time
    time_saved = ((normal_time - mmap_time) / normal_time) * 100
    
    print(f"\nğŸ“ˆ BENCHMARK SONUÃ‡LARI:")
    print(f"   ğŸŒ Normal Array: {normal_time:.4f} saniye")
    print(f"   ğŸš€ Memory Map: {mmap_time:.4f} saniye")
    print(f"   âš¡ HÄ±zlanma: {speedup:.2f}x")
    print(f"   ğŸ’° Zaman tasarrufu: {time_saved:.1f}%")
    
    return {
        'normal_time': normal_time,
        'mmap_time': mmap_time,
        'speedup': speedup,
        'time_saved': time_saved,
        'n_iterations': n_iterations
    }


# =============================================================================
# Profiling AraÃ§larÄ± - Performans Analizi
# =============================================================================

def get_profiling_status():
    """Profiling araÃ§larÄ±nÄ±n kullanÄ±labilirlik durumunu dÃ¶ndÃ¼rÃ¼r"""
    if PROFILING_AVAILABLE:
        print("âœ… Profiling araÃ§larÄ± aktif!")
        print(f"   ğŸ” cProfile, pstats, psutil, tracemalloc kullanÄ±labilir")
        return True
    else:
        print("âŒ Profiling araÃ§larÄ± devre dÄ±ÅŸÄ±!")
        print("   ğŸ“¦ Kurulum: pip install psutil")
        return False


def profile_function(func, *args, **kwargs):
    """
    Fonksiyonu cProfile ile profil eder.
    
    Parameters
    ----------
    func : callable
        Profil edilecek fonksiyon
    *args, **kwargs
        Fonksiyon parametreleri
        
    Returns
    -------
    tuple
        (sonuÃ§, profil_raporu)
    """
    if not PROFILING_AVAILABLE:
        print("âŒ Profiling araÃ§larÄ± bulunamadÄ±!")
        return func(*args, **kwargs), None
    
    print(f"ğŸ” {func.__name__} fonksiyonu profil ediliyor...")
    
    # Profiler baÅŸlat
    profiler = cProfile.Profile()
    profiler.enable()
    
    # Fonksiyonu Ã§alÄ±ÅŸtÄ±r
    start_time = time.time()
    result = func(*args, **kwargs)
    execution_time = time.time() - start_time
    
    # Profiler durdur
    profiler.disable()
    
    # Profil raporu oluÅŸtur
    s = io.StringIO()
    stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
    stats.print_stats(20)  # Ä°lk 20 satÄ±r
    
    print(f"âœ… Profil tamamlandÄ±! Ã‡alÄ±ÅŸma sÃ¼resi: {execution_time:.4f} saniye")
    
    return result, s.getvalue()


def get_memory_usage():
    """
    Mevcut bellek kullanÄ±mÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.
    
    Returns
    -------
    dict
        Bellek kullanÄ±m bilgileri
    """
    if not PROFILING_AVAILABLE:
        print("âŒ psutil bulunamadÄ±!")
        return {}
    
    try:
        process = psutil.Process()
        memory_info = process.memory_info()
        
        # Sistem bellek bilgileri
        system_memory = psutil.virtual_memory()
        
        memory_stats = {
            'rss_mb': memory_info.rss / 1024 / 1024,  # Resident Set Size
            'vms_mb': memory_info.vms / 1024 / 1024,  # Virtual Memory Size
            'percent': process.memory_percent(),
            'system_total_gb': system_memory.total / 1024 / 1024 / 1024,
            'system_available_gb': system_memory.available / 1024 / 1024 / 1024,
            'system_percent': system_memory.percent
        }
        
        print("ğŸ’¾ Bellek kullanÄ±mÄ±:")
        print(f"   ğŸ“Š RSS: {memory_stats['rss_mb']:.2f} MB")
        print(f"   ğŸ“Š VMS: {memory_stats['vms_mb']:.2f} MB")
        print(f"   ğŸ“Š Process: {memory_stats['percent']:.1f}%")
        print(f"   ğŸ“Š Sistem: {memory_stats['system_percent']:.1f}%")
        
        return memory_stats
        
    except Exception as e:
        print(f"âŒ Bellek bilgisi alÄ±namadÄ±: {e}")
        return {}


def start_memory_tracking():
    """Bellek takibini baÅŸlatÄ±r"""
    if not PROFILING_AVAILABLE:
        print("âŒ tracemalloc bulunamadÄ±!")
        return None
    
    try:
        tracemalloc.start()
        print("âœ… Bellek takibi baÅŸlatÄ±ldÄ±!")
        return True
    except Exception as e:
        print(f"âŒ Bellek takibi baÅŸlatÄ±lamadÄ±: {e}")
        return None


def get_memory_snapshot():
    """
    Bellek anlÄ±k gÃ¶rÃ¼ntÃ¼sÃ¼ alÄ±r.
    
    Returns
    -------
    dict
        Bellek anlÄ±k gÃ¶rÃ¼ntÃ¼ bilgileri
    """
    if not PROFILING_AVAILABLE:
        print("âŒ tracemalloc bulunamadÄ±!")
        return {}
    
    try:
        snapshot = tracemalloc.take_snapshot()
        
        # En Ã§ok bellek kullanan dosyalar
        top_stats = snapshot.statistics('lineno')
        
        memory_info = {
            'total_allocated_mb': sum(stat.size for stat in top_stats) / 1024 / 1024,
            'top_files': []
        }
        
        print("ğŸ“¸ Bellek anlÄ±k gÃ¶rÃ¼ntÃ¼sÃ¼:")
        print(f"   ğŸ’¾ Toplam ayrÄ±lan: {memory_info['total_allocated_mb']:.2f} MB")
        
        # Ä°lk 5 dosyayÄ± gÃ¶ster
        for i, stat in enumerate(top_stats[:5]):
            file_info = {
                'file': stat.traceback.format()[-1],
                'size_mb': stat.size / 1024 / 1024,
                'count': stat.count
            }
            memory_info['top_files'].append(file_info)
            print(f"   ğŸ“ {file_info['file']}: {file_info['size_mb']:.2f} MB ({file_info['count']} blok)")
        
        return memory_info
        
    except Exception as e:
        print(f"âŒ Bellek anlÄ±k gÃ¶rÃ¼ntÃ¼sÃ¼ alÄ±namadÄ±: {e}")
        return {}


def benchmark_suite(df, operations=['info', 'describe', 'corr', 'outliers']):
    """
    KapsamlÄ± benchmark testi.
    
    Parameters
    ----------
    df : pd.DataFrame
        Test edilecek veri seti
    operations : list
        Test edilecek operasyonlar
        
    Returns
    -------
    dict
        Benchmark sonuÃ§larÄ±
    """
    print("ğŸ KAPSAMLI BENCHMARK SUITE BAÅLIYOR...")
    print(f"   ğŸ“Š Veri seti boyutu: {df.shape}")
    print(f"   ğŸ’¾ Bellek kullanÄ±mÄ±: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
    
    # Bellek takibini baÅŸlat
    start_memory_tracking()
    
    results = {}
    total_start_time = time.time()
    
    for op in operations:
        print(f"\nğŸ” {op.upper()} operasyonu test ediliyor...")
        
        # Bellek Ã¶ncesi
        memory_before = get_memory_snapshot()
        
        # Operasyonu Ã§alÄ±ÅŸtÄ±r
        start_time = time.time()
        
        if op == 'info':
            result = get_data_info(df)
        elif op == 'describe':
            result = df.describe()
        elif op == 'corr':
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 1:
                result = df[numeric_cols].corr()
            else:
                result = None
        elif op == 'outliers':
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                result = detect_outliers(df[numeric_cols])
            else:
                result = None
        
        execution_time = time.time() - start_time
        
        # Bellek sonrasÄ±
        memory_after = get_memory_snapshot()
        
        # SonuÃ§larÄ± kaydet
        results[op] = {
            'execution_time': execution_time,
            'memory_before': memory_before.get('total_allocated_mb', 0),
            'memory_after': memory_after.get('total_allocated_mb', 0),
            'memory_increase': memory_after.get('total_allocated_mb', 0) - memory_before.get('total_allocated_mb', 0)
        }
        
        print(f"   â±ï¸  SÃ¼re: {execution_time:.4f} saniye")
        print(f"   ğŸ’¾ Bellek artÄ±ÅŸÄ±: {results[op]['memory_increase']:.2f} MB")
    
    total_time = time.time() - total_start_time
    
    # Genel Ã¶zet
    print(f"\nğŸ“ˆ GENEL BENCHMARK SONUÃ‡LARI:")
    print(f"   â±ï¸  Toplam sÃ¼re: {total_time:.4f} saniye")
    print(f"   ğŸ” Test edilen operasyon: {len(operations)}")
    
    # En hÄ±zlÄ± ve en yavaÅŸ operasyonlar
    execution_times = {op: results[op]['execution_time'] for op in results}
    fastest_op = min(execution_times, key=execution_times.get)
    slowest_op = max(execution_times, key=execution_times.get)
    
    print(f"   ğŸš€ En hÄ±zlÄ±: {fastest_op} ({execution_times[fastest_op]:.4f}s)")
    print(f"   ğŸŒ En yavaÅŸ: {slowest_op} ({execution_times[slowest_op]:.4f}s)")
    
    return results


# =============================================================================
# Async/Await ile Asenkron Veri Analizi
# =============================================================================

def get_async_status():
    """Async/await kullanÄ±labilirlik durumunu dÃ¶ndÃ¼rÃ¼r"""
    if ASYNC_AVAILABLE:
        print("âœ… Async/await desteÄŸi aktif!")
        print(f"   ğŸš€ Asenkron veri analizi yapÄ±labilir")
        return True
    else:
        print("âŒ Async/await desteÄŸi devre dÄ±ÅŸÄ±!")
        print("   ğŸ“¦ Kurulum: pip install aiofiles aiohttp")
        return False


async def async_analyze_dataset(df, operations=['info', 'describe', 'corr']):
    """
    Veri setini asenkron olarak analiz eder.
    
    Parameters
    ----------
    df : pd.DataFrame
        Analiz edilecek veri seti
    operations : list
        YapÄ±lacak operasyonlar
        
    Returns
    -------
    dict
        Analiz sonuÃ§larÄ±
    """
    if not ASYNC_AVAILABLE:
        print("âŒ Async/await bulunamadÄ±! Senkron analiz kullanÄ±lÄ±yor.")
        return {}
    
    print("ğŸš€ Asenkron veri analizi baÅŸlÄ±yor...")
    print(f"   ğŸ“Š Veri seti boyutu: {df.shape}")
    
    results = {}
    start_time = time.time()
    
    # Asenkron operasyonlar
    async def process_operation(op):
        print(f"   ğŸ” {op.upper()} operasyonu iÅŸleniyor...")
        
        # SimÃ¼le edilmiÅŸ asenkron iÅŸlem
        await asyncio.sleep(0.1)
        
        if op == 'info':
            result = get_data_info(df)
        elif op == 'describe':
            result = df.describe()
        elif op == 'corr':
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 1:
                result = df[numeric_cols].corr()
            else:
                result = None
        
        return op, result
    
    # TÃ¼m operasyonlarÄ± paralel olarak Ã§alÄ±ÅŸtÄ±r
    tasks = [process_operation(op) for op in operations]
    completed_results = await asyncio.gather(*tasks)
    
    # SonuÃ§larÄ± dÃ¼zenle
    for op, result in completed_results:
        results[op] = result
    
    total_time = time.time() - start_time
    print(f"âœ… Asenkron analiz tamamlandÄ±! Toplam sÃ¼re: {total_time:.4f} saniye")
    
    return results


async def async_data_loading(file_paths):
    """
    Birden fazla dosyayÄ± asenkron olarak yÃ¼kler.
    
    Parameters
    ----------
    file_paths : list
        YÃ¼klenecek dosya yollarÄ±
        
    Returns
    -------
    dict
        YÃ¼klenen veri setleri
    """
    if not ASYNC_AVAILABLE:
        print("âŒ Async/await bulunamadÄ±!")
        return {}
    
    print(f"ğŸ“ {len(file_paths)} dosya asenkron olarak yÃ¼kleniyor...")
    
    async def load_file(file_path):
        try:
            # SimÃ¼le edilmiÅŸ dosya yÃ¼kleme
            await asyncio.sleep(0.2)
            
            # GerÃ§ek uygulamada burada dosya yÃ¼kleme olacak
            # df = pd.read_csv(file_path) gibi
            
            return file_path, f"Loaded data from {file_path}"
        except Exception as e:
            return file_path, f"Error loading {file_path}: {e}"
    
    # TÃ¼m dosyalarÄ± paralel olarak yÃ¼kle
    tasks = [load_file(path) for path in file_paths]
    results = await asyncio.gather(*tasks)
    
    return dict(results)


async def async_benchmark_async_vs_sync(df, operations=['info', 'describe', 'corr']):
    """
    Asenkron vs senkron performans karÅŸÄ±laÅŸtÄ±rmasÄ±.
    
    Parameters
    ----------
    df : pd.DataFrame
        Test edilecek veri seti
    operations : list
        Test edilecek operasyonlar
        
    Returns
    -------
    dict
        Benchmark sonuÃ§larÄ±
    """
    if not ASYNC_AVAILABLE:
        print("âŒ Async/await bulunamadÄ±! Benchmark yapÄ±lamÄ±yor.")
        return {}
    
    print("ğŸ Async vs Sync Benchmark BaÅŸlÄ±yor...")
    
    # Senkron test
    print("\nğŸ” Senkron test...")
    start_time = time.time()
    sync_results = {}
    for op in operations:
        if op == 'info':
            sync_results[op] = get_data_info(df)
        elif op == 'describe':
            sync_results[op] = df.describe()
        elif op == 'corr':
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 1:
                sync_results[op] = df[numeric_cols].corr()
    sync_time = time.time() - start_time
    
    # Asenkron test
    print("ğŸš€ Asenkron test...")
    start_time = time.time()
    async_results = await async_analyze_dataset(df, operations)
    async_time = time.time() - start_time
    
    # SonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±r
    speedup = sync_time / async_time
    time_saved = ((sync_time - async_time) / sync_time) * 100
    
    print(f"\nğŸ“ˆ BENCHMARK SONUÃ‡LARI:")
    print(f"   ğŸŒ Senkron: {sync_time:.4f} saniye")
    print(f"   ğŸš€ Asenkron: {async_time:.4f} saniye")
    print(f"   âš¡ HÄ±zlanma: {speedup:.2f}x")
    print(f"   ğŸ’° Zaman tasarrufu: {time_saved:.1f}%")
    
    return {
        'sync_time': sync_time,
        'async_time': async_time,
        'speedup': speedup,
        'time_saved': time_saved
    }


def run_async_example():
    """
    Async/await kullanÄ±m Ã¶rneÄŸi.
    """
    if not ASYNC_AVAILABLE:
        print("âŒ Async/await bulunamadÄ±!")
        return
    
    print("ğŸš€ Async/await Ã¶rneÄŸi Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
    
    # Test veri seti oluÅŸtur
    df = pd.DataFrame({
        'yas': np.random.normal(30, 10, 10000),
        'maas': np.random.normal(50000, 15000, 10000),
        'deneyim': np.random.normal(5, 3, 10000)
    })
    
    # Asenkron analiz Ã§alÄ±ÅŸtÄ±r
    async def main():
        # Veri analizi
        results = await async_analyze_dataset(df, ['info', 'describe', 'corr'])
        
        # Dosya yÃ¼kleme simÃ¼lasyonu
        file_paths = ['data1.csv', 'data2.csv', 'data3.csv']
        loaded_data = await async_data_loading(file_paths)
        
        # Benchmark
        benchmark = await async_benchmark_async_vs_sync(df)
        
        return results, loaded_data, benchmark
    
    # Event loop Ã§alÄ±ÅŸtÄ±r
    try:
        results, loaded_data, benchmark = asyncio.run(main())
        print("âœ… Async/await Ã¶rneÄŸi baÅŸarÄ±yla tamamlandÄ±!")
        return results, loaded_data, benchmark
    except Exception as e:
        print(f"âŒ Async/await Ã¶rneÄŸi hatasÄ±: {e}")
        return None, None, None


# =============================================================================
# Dask ile BÃ¼yÃ¼k Veri Analizi
# =============================================================================

def get_dask_status():
    """Dask kullanÄ±labilirlik durumunu dÃ¶ndÃ¼rÃ¼r"""
    if DASK_AVAILABLE:
        print("âœ… Dask bÃ¼yÃ¼k veri analizi aktif!")
        print(f"   ğŸš€ GB-TB boyutunda veri setleri analiz edilebilir")
        return True
    else:
        print("âŒ Dask bÃ¼yÃ¼k veri analizi devre dÄ±ÅŸÄ±!")
        print("   ğŸ“¦ Kurulum: pip install dask[complete]")
        return False


def create_dask_client(n_workers=None, memory_limit='2GB'):
    """
    Dask client oluÅŸturur.
    
    Parameters
    ----------
    n_workers : int, optional
        Worker sayÄ±sÄ± (None ise otomatik)
    memory_limit : str, optional
        Worker baÅŸÄ±na bellek limiti
        
    Returns
    -------
    dask.distributed.Client or None
        Dask client veya None
    """
    if not DASK_AVAILABLE:
        print("âŒ Dask bulunamadÄ±! Client oluÅŸturulamÄ±yor.")
        return None
    
    try:
        # Local cluster oluÅŸtur
        cluster = LocalCluster(
            n_workers=n_workers,
            memory_limit=memory_limit,
            processes=True
        )
        
        # Client oluÅŸtur
        client = Client(cluster)
        
        print(f"âœ… Dask client oluÅŸturuldu!")
        print(f"   ğŸ”§ Worker sayÄ±sÄ±: {len(client.scheduler_info()['workers'])}")
        print(f"   ğŸ’¾ Bellek limiti: {memory_limit}")
        print(f"   ğŸŒ Dashboard: {client.dashboard_link}")
        
        return client
        
    except Exception as e:
        print(f"âŒ Dask client oluÅŸturulamadÄ±: {e}")
        return None


def convert_to_dask(df, npartitions=None):
    """
    Pandas DataFrame'i Dask DataFrame'e Ã§evirir.
    
    Parameters
    ----------
    df : pd.DataFrame
        Ã‡evrilecek DataFrame
    npartitions : int, optional
        Partition sayÄ±sÄ± (None ise otomatik)
        
    Returns
    -------
    dask.dataframe.DataFrame or pd.DataFrame
        Dask DataFrame veya orijinal DataFrame
    """
    if not DASK_AVAILABLE:
        print("âš ï¸  Dask bulunamadÄ±! Orijinal DataFrame dÃ¶ndÃ¼rÃ¼lÃ¼yor.")
        return df
    
    if npartitions is None:
        # Otomatik partition sayÄ±sÄ± hesapla
        npartitions = max(1, len(df) // 10000)  # 10K satÄ±r per partition
    
    try:
        ddf = dd.from_pandas(df, npartitions=npartitions)
        print(f"âœ… DataFrame Dask'a Ã§evrildi!")
        print(f"   ğŸ“Š Partition sayÄ±sÄ±: {ddf.npartitions}")
        print(f"   ğŸ’¾ Tahmini bellek: {ddf.memory_usage_per_partition().sum().compute() / 1024 / 1024:.2f} MB")
        return ddf
        
    except Exception as e:
        print(f"âš ï¸  Dask'a Ã§evirme hatasÄ±: {e}")
        return df


def dask_analyze_large_dataset(df, chunk_size='100MB', show_progress=True):
    """
    BÃ¼yÃ¼k veri setini Dask ile analiz eder.
    
    Parameters
    ----------
    df : pd.DataFrame or dask.dataframe.DataFrame
        Analiz edilecek veri seti
    chunk_size : str, optional
        Chunk boyutu
    show_progress : bool, optional
        Ä°lerleme Ã§ubuÄŸu gÃ¶ster
        
    Returns
    -------
    dict
        Analiz sonuÃ§larÄ±
    """
    if not DASK_AVAILABLE:
        print("âŒ Dask bulunamadÄ±! Standart analiz kullanÄ±lÄ±yor.")
        return {}
    
    print("ğŸš€ Dask ile bÃ¼yÃ¼k veri analizi baÅŸlÄ±yor...")
    
    # DataFrame'i Dask'a Ã§evir
    if not isinstance(df, dd.DataFrame):
        df = convert_to_dask(df)
    
    if not isinstance(df, dd.DataFrame):
        print("âŒ Dask DataFrame oluÅŸturulamadÄ±!")
        return {}
    
    results = {}
    
    try:
        # Veri seti bilgileri
        print("ğŸ“Š Veri seti bilgileri hesaplanÄ±yor...")
        results['shape'] = df.shape.compute()
        results['columns'] = df.columns.tolist()
        results['dtypes'] = df.dtypes.to_dict()
        
        # SayÄ±sal sÃ¼tunlar
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        if numeric_cols:
            print("ğŸ”¢ SayÄ±sal analiz yapÄ±lÄ±yor...")
            
            # Temel istatistikler
            numeric_df = df[numeric_cols]
            stats = numeric_df.describe().compute()
            results['numeric_stats'] = stats.to_dict()
            
            # Korelasyon matrisi (bÃ¼yÃ¼k veri iÃ§in Ã¶rnekleme)
            if len(numeric_cols) > 1:
                print("ğŸ”— Korelasyon analizi yapÄ±lÄ±yor...")
                # BÃ¼yÃ¼k veri iÃ§in Ã¶rnekleme
                sample_size = min(10000, len(df))
                sample_df = df.sample(n=sample_size, random_state=42)
                corr_matrix = sample_df[numeric_cols].corr().compute()
                results['correlation_matrix'] = corr_matrix.to_dict()
        
        # Kategorik sÃ¼tunlar
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        if categorical_cols:
            print("ğŸ“ Kategorik analiz yapÄ±lÄ±yor...")
            
            categorical_stats = {}
            for col in categorical_cols[:5]:  # Ä°lk 5 kategorik sÃ¼tun
                value_counts = df[col].value_counts().head(10).compute()
                categorical_stats[col] = value_counts.to_dict()
            
            results['categorical_stats'] = categorical_stats
        
        # Eksik deÄŸer analizi
        print("â“ Eksik deÄŸer analizi yapÄ±lÄ±yor...")
        missing_counts = df.isnull().sum().compute()
        results['missing_values'] = missing_counts.to_dict()
        
        # Bellek kullanÄ±mÄ±
        memory_usage = df.memory_usage_per_partition().sum().compute()
        results['memory_usage_mb'] = memory_usage / 1024 / 1024
        
        print("âœ… Dask analizi tamamlandÄ±!")
        
    except Exception as e:
        print(f"âŒ Dask analizi hatasÄ±: {e}")
        results['error'] = str(e)
    
    return results


def benchmark_dask_vs_pandas(df, operations=['describe', 'corr', 'groupby']):
    """
    Dask vs Pandas performans karÅŸÄ±laÅŸtÄ±rmasÄ±.
    
    Parameters
    ----------
    df : pd.DataFrame
        Test edilecek veri seti
    operations : list
        Test edilecek operasyonlar
        
    Returns
    -------
    dict
        Benchmark sonuÃ§larÄ±
    """
    if not DASK_AVAILABLE:
        print("âŒ Dask bulunamadÄ±! Benchmark yapÄ±lamÄ±yor.")
        return {}
    
    print("ğŸ Dask vs Pandas Benchmark BaÅŸlÄ±yor...")
    print(f"   ğŸ“Š Veri seti boyutu: {df.shape}")
    
    # Dask DataFrame oluÅŸtur
    ddf = convert_to_dask(df)
    if not isinstance(ddf, dd.DataFrame):
        print("âŒ Dask DataFrame oluÅŸturulamadÄ±!")
        return {}
    
    results = {}
    
    for op in operations:
        if op == 'describe':
            print(f"\nğŸ” {op} operasyonu test ediliyor...")
            
            # Pandas
            start_time = time.time()
            pandas_result = df.describe()
            pandas_time = time.time() - start_time
            
            # Dask
            start_time = time.time()
            dask_result = ddf.describe().compute()
            dask_time = time.time() - start_time
            
            speedup = pandas_time / dask_time
            results[op] = {
                'pandas_time': pandas_time,
                'dask_time': dask_time,
                'speedup': speedup
            }
            
            print(f"   ğŸŒ Pandas: {pandas_time:.4f}s")
            print(f"   ğŸš€ Dask: {dask_time:.4f}s")
            print(f"   âš¡ HÄ±zlanma: {speedup:.2f}x")
        
        elif op == 'corr' and len(df.select_dtypes(include=['number']).columns) > 1:
            print(f"\nğŸ” {op} operasyonu test ediliyor...")
            
            numeric_cols = df.select_dtypes(include=['number']).columns
            
            # Pandas
            start_time = time.time()
            pandas_result = df[numeric_cols].corr()
            pandas_time = time.time() - start_time
            
            # Dask
            start_time = time.time()
            dask_result = ddf[numeric_cols].corr().compute()
            dask_time = time.time() - start_time
            
            speedup = pandas_time / dask_time
            results[op] = {
                'pandas_time': pandas_time,
                'dask_time': dask_time,
                'speedup': speedup
            }
            
            print(f"   ğŸŒ Pandas: {pandas_time:.4f}s")
            print(f"   ğŸš€ Dask: {dask_time:.4f}s")
            print(f"   âš¡ HÄ±zlanma: {speedup:.2f}x")
    
    return results


def create_large_test_dataset(rows=1000000, cols=10):
    """
    Test iÃ§in bÃ¼yÃ¼k veri seti oluÅŸturur.
    
    Parameters
    ----------
    rows : int
        SatÄ±r sayÄ±sÄ±
    cols : int
        SÃ¼tun sayÄ±sÄ±
        
    Returns
    -------
    pd.DataFrame
        Test veri seti
    """
    print(f"ğŸ“Š {rows:,} satÄ±r x {cols} sÃ¼tun test veri seti oluÅŸturuluyor...")
    
    # SayÄ±sal sÃ¼tunlar
    data = {}
    for i in range(cols):
        if i % 3 == 0:  # Float
            data[f'float_{i}'] = np.random.normal(0, 1, rows)
        elif i % 3 == 1:  # Integer
            data[f'int_{i}'] = np.random.randint(0, 1000, rows)
        else:  # Categorical
            categories = ['A', 'B', 'C', 'D', 'E']
            data[f'cat_{i}'] = np.random.choice(categories, rows)
    
    df = pd.DataFrame(data)
    
    print(f"âœ… Test veri seti oluÅŸturuldu!")
    print(f"   ğŸ’¾ Bellek kullanÄ±mÄ±: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
    
    return df


# =============================================================================
# GPU Acceleration - CUDA ile HÄ±zlandÄ±rÄ±lmÄ±ÅŸ Analiz
# =============================================================================

def get_gpu_status():
    """GPU acceleration kullanÄ±labilirlik durumunu dÃ¶ndÃ¼rÃ¼r"""
    if GPU_AVAILABLE:
        try:
            # GPU bilgilerini al
            gpu_memory = cp.cuda.runtime.memGetInfo()
            gpu_free_mb = gpu_memory[0] / 1024 / 1024
            gpu_total_mb = gpu_memory[1] / 1024 / 1024
            gpu_used_mb = gpu_total_mb - gpu_free_mb
            
            print("âœ… GPU acceleration aktif!")
            print(f"   ğŸš€ CUDA ile hÄ±zlandÄ±rÄ±lmÄ±ÅŸ analiz yapÄ±labilir")
            print(f"   ğŸ’¾ GPU Bellek: {gpu_used_mb:.1f} MB / {gpu_total_mb:.1f} MB")
            return True
        except Exception as e:
            print(f"âš ï¸  GPU bilgisi alÄ±namadÄ±: {e}")
            return True
    else:
        print("âŒ GPU acceleration devre dÄ±ÅŸÄ±!")
        print("   ğŸ“¦ Kurulum: pip install cupy-cuda11x (CUDA sÃ¼rÃ¼mÃ¼nÃ¼ze gÃ¶re)")
        return False


def convert_to_gpu_array(data):
    """
    NumPy array'i GPU array'e Ã§evirir.
    
    Parameters
    ----------
    data : numpy.ndarray
        GPU'ya aktarÄ±lacak veri
        
    Returns
    -------
    cupy.ndarray or numpy.ndarray
        GPU array veya orijinal array
    """
    if not GPU_AVAILABLE:
        print("âš ï¸  GPU bulunamadÄ±! CPU array dÃ¶ndÃ¼rÃ¼lÃ¼yor.")
        return data
    
    try:
        gpu_array = cp.asarray(data)
        print(f"âœ… Veri GPU'ya aktarÄ±ldÄ±!")
        print(f"   ğŸ“Š Boyut: {gpu_array.shape}")
        print(f"   ğŸ’¾ GPU Bellek: {gpu_array.nbytes / 1024 / 1024:.2f} MB")
        return gpu_array
    except Exception as e:
        print(f"âŒ GPU'ya aktarma hatasÄ±: {e}")
        return data


def gpu_correlation_matrix(data):
    """
    GPU ile hÄ±zlandÄ±rÄ±lmÄ±ÅŸ korelasyon matrisi hesaplama.
    
    Parameters
    ----------
    data : cupy.ndarray or numpy.ndarray
        GPU'da analiz edilecek veri
        
    Returns
    -------
    cupy.ndarray or numpy.ndarray
        Korelasyon matrisi
    """
    if not GPU_AVAILABLE:
        print("âŒ GPU bulunamadÄ±! CPU korelasyon hesaplanÄ±yor.")
        return np.corrcoef(data, rowvar=False)
    
    try:
        # GPU'ya aktar
        if not isinstance(data, cp.ndarray):
            data = convert_to_gpu_array(data)
        
        if not isinstance(data, cp.ndarray):
            return np.corrcoef(data, rowvar=False)
        
        # GPU'da korelasyon hesapla
        print("ğŸš€ GPU'da korelasyon hesaplanÄ±yor...")
        
        # Veriyi normalize et
        data_centered = data - cp.mean(data, axis=0)
        
        # Kovaryans matrisi
        cov_matrix = cp.dot(data_centered.T, data_centered) / (data.shape[0] - 1)
        
        # Standart sapmalar
        std_devs = cp.sqrt(cp.diag(cov_matrix))
        
        # Korelasyon matrisi
        corr_matrix = cov_matrix / cp.outer(std_devs, std_devs)
        
        # NaN deÄŸerleri dÃ¼zelt
        corr_matrix = cp.nan_to_num(corr_matrix, nan=0.0)
        
        print("âœ… GPU korelasyon hesaplandÄ±!")
        return corr_matrix
        
    except Exception as e:
        print(f"âŒ GPU korelasyon hatasÄ±: {e}")
        # CPU'ya geri dÃ¶n
        if isinstance(data, cp.ndarray):
            data = cp.asnumpy(data)
        return np.corrcoef(data, rowvar=False)


def gpu_outlier_detection(data, method='iqr', threshold=1.5):
    """
    GPU ile hÄ±zlandÄ±rÄ±lmÄ±ÅŸ aykÄ±rÄ± deÄŸer tespiti.
    
    Parameters
    ----------
    data : cupy.ndarray or numpy.ndarray
        GPU'da analiz edilecek veri
    method : str
        'iqr' veya 'zscore'
    threshold : float
        AykÄ±rÄ± deÄŸer eÅŸiÄŸi
        
    Returns
    -------
    cupy.ndarray or numpy.ndarray
        Boolean mask - True deÄŸerler aykÄ±rÄ± deÄŸerleri gÃ¶sterir
    """
    if not GPU_AVAILABLE:
        print("âŒ GPU bulunamadÄ±! CPU aykÄ±rÄ± deÄŸer tespiti yapÄ±lÄ±yor.")
        return detect_outliers(pd.DataFrame(data), method, threshold)
    
    try:
        # GPU'ya aktar
        if not isinstance(data, cp.ndarray):
            data = convert_to_gpu_array(data)
        
        if not isinstance(data, cp.ndarray):
            return detect_outliers(pd.DataFrame(data), method, threshold)
        
        print("ğŸš€ GPU'da aykÄ±rÄ± deÄŸer tespiti yapÄ±lÄ±yor...")
        
        outliers = cp.zeros_like(data, dtype=cp.bool_)
        
        if method == 'iqr':
            for j in range(data.shape[1]):
                col = data[:, j]
                
                # Q1, Q3 ve IQR hesapla
                q1 = cp.percentile(col, 25)
                q3 = cp.percentile(col, 75)
                iqr = q3 - q1
                
                lower_bound = q1 - threshold * iqr
                upper_bound = q3 + threshold * iqr
                
                outliers[:, j] = (col < lower_bound) | (col > upper_bound)
        
        elif method == 'zscore':
            for j in range(data.shape[1]):
                col = data[:, j]
                
                # Q1, Q3 ve IQR hesapla
                q1 = cp.percentile(col, 25)
                q3 = cp.percentile(col, 75)
                iqr = q3 - q1
                
                lower_bound = q1 - threshold * iqr
                upper_bound = q3 + threshold * iqr
                
                outliers[:, j] = (col < lower_bound) | (col > upper_bound)
        
        print("âœ… GPU aykÄ±rÄ± deÄŸer tespiti tamamlandÄ±!")
        return outliers
        
    except Exception as e:
        print(f"âŒ GPU aykÄ±rÄ± deÄŸer tespiti hatasÄ±: {e}")
        # CPU'ya geri dÃ¶n
        if isinstance(data, cp.ndarray):
            data = cp.asnumpy(data)
        return detect_outliers(pd.DataFrame(data), method, threshold)


def gpu_summary_stats(data):
    """
    GPU ile hÄ±zlandÄ±rÄ±lmÄ±ÅŸ istatistiksel Ã¶zet.
    
    Parameters
    ----------
    data : cupy.ndarray or numpy.ndarray
        GPU'da analiz edilecek veri
        
    Returns
    -------
    dict
        Her sÃ¼tun iÃ§in istatistikler
    """
    if not GPU_AVAILABLE:
        print("âŒ GPU bulunamadÄ±! CPU istatistikleri hesaplanÄ±yor.")
        return fast_summary_stats(data)
    
    try:
        # GPU'ya aktar
        if not isinstance(data, cp.ndarray):
            data = convert_to_gpu_array(data)
        
        if not isinstance(data, cp.ndarray):
            return fast_summary_stats(data)
        
        print("ğŸš€ GPU'da istatistikler hesaplanÄ±yor...")
        
        stats = {}
        for j in range(data.shape[1]):
            col = data[:, j]
            
            # Temel istatistikler
            mean_val = float(cp.mean(col))
            std_val = float(cp.mean(col))
            min_val = float(cp.min(col))
            max_val = float(cp.max(col))
            
            # Medyan hesapla
            median_val = float(cp.median(col))
            
            # Q1 ve Q3 hesapla
            q1_val = float(cp.percentile(col, 25))
            q3_val = float(cp.percentile(col, 75))
            
            stats[j] = {
                'mean': mean_val,
                'median': median_val,
                'std': std_val,
                'min': min_val,
                'max': max_val,
                'q1': q1_val,
                'q3': q3_val
            }
        
        print("âœ… GPU istatistikleri hesaplandÄ±!")
        return stats
        
    except Exception as e:
        print(f"âŒ GPU istatistik hatasÄ±: {e}")
        # CPU'ya geri dÃ¶n
        if isinstance(data, cp.ndarray):
            data = cp.asnumpy(data)
        return fast_summary_stats(data)


def benchmark_gpu_vs_cpu(data, operations=['corr', 'outliers', 'stats']):
    """
    GPU vs CPU performans karÅŸÄ±laÅŸtÄ±rmasÄ±.
    
    Parameters
    ----------
    data : numpy.ndarray
        Test edilecek veri
    operations : list
        Test edilecek operasyonlar
        
    Returns
    -------
    dict
        Benchmark sonuÃ§larÄ±
    """
    if not GPU_AVAILABLE:
        print("âŒ GPU bulunamadÄ±! Benchmark yapÄ±lamÄ±yor.")
        return {}
    
    print("ğŸ GPU vs CPU Benchmark BaÅŸlÄ±yor...")
    print(f"   ğŸ“Š Veri boyutu: {data.shape}")
    print(f"   ğŸ’¾ Veri boyutu: {data.nbytes / 1024 / 1024:.2f} MB")
    
    results = {}
    
    for op in operations:
        print(f"\nğŸ” {op.upper()} operasyonu test ediliyor...")
        
        if op == 'corr':
            # CPU korelasyon testi
            print("   ğŸŒ CPU korelasyon testi...")
            start_time = time.time()
            cpu_result = np.corrcoef(data, rowvar=False)
            cpu_time = time.time() - start_time
            
            # GPU korelasyon testi
            print("   ğŸš€ GPU korelasyon testi...")
            start_time = time.time()
            gpu_result = gpu_correlation_matrix(data)
            gpu_time = time.time() - start_time
            
            # SonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±r
            if cpu_time > 0:
                speedup = cpu_time / gpu_time
                time_saved = ((cpu_time - gpu_time) / cpu_time) * 100
            else:
                speedup = 0
                time_saved = 0
            
            results[op] = {
                'cpu_time': cpu_time,
                'gpu_time': gpu_time,
                'speedup': speedup,
                'time_saved': time_saved
            }
            
            print(f"   ğŸŒ CPU: {cpu_time:.4f}s")
            print(f"   ğŸš€ GPU: {gpu_time:.4f}s")
            print(f"   âš¡ HÄ±zlanma: {speedup:.2f}x")
            print(f"   ğŸ’° Zaman tasarrufu: {time_saved:.1f}%")
        
        elif op == 'outliers':
            # CPU aykÄ±rÄ± deÄŸer testi
            print("   ğŸŒ CPU aykÄ±rÄ± deÄŸer testi...")
            start_time = time.time()
            cpu_result = detect_outliers(pd.DataFrame(data))
            cpu_time = time.time() - start_time
            
            # GPU aykÄ±rÄ± deÄŸer testi
            print("   ğŸš€ GPU aykÄ±rÄ± deÄŸer testi...")
            start_time = time.time()
            gpu_result = gpu_outlier_detection(data)
            gpu_time = time.time() - start_time
            
            # SonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±r
            if cpu_time > 0:
                speedup = cpu_time / gpu_time
                time_saved = ((cpu_time - gpu_time) / cpu_time) * 100
            else:
                speedup = 0
                time_saved = 0
            
            results[op] = {
                'cpu_time': cpu_time,
                'gpu_time': gpu_time,
                'speedup': speedup,
                'time_saved': time_saved
            }
            
            print(f"   ğŸŒ CPU: {cpu_time:.4f}s")
            print(f"   ğŸš€ GPU: {gpu_time:.4f}s")
            print(f"   âš¡ HÄ±zlanma: {speedup:.2f}x")
            print(f"   ğŸ’° Zaman tasarrufu: {time_saved:.1f}%")
        
        elif op == 'stats':
            # CPU istatistik testi
            print("   ğŸŒ CPU istatistik testi...")
            start_time = time.time()
            cpu_result = fast_summary_stats(data)
            cpu_time = time.time() - start_time
            
            # GPU istatistik testi
            print("   ğŸš€ GPU istatistik testi...")
            start_time = time.time()
            gpu_result = gpu_summary_stats(data)
            gpu_time = time.time() - start_time
            
            # SonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±r
            if cpu_time > 0:
                speedup = cpu_time / gpu_time
                time_saved = ((cpu_time - gpu_time) / cpu_time) * 100
            else:
                speedup = 0
                time_saved = 0
            
            results[op] = {
                'cpu_time': cpu_time,
                'gpu_time': gpu_time,
                'speedup': speedup,
                'time_saved': time_saved
            }
            
            print(f"   ğŸŒ CPU: {cpu_time:.4f}s")
            print(f"   ğŸš€ GPU: {gpu_time:.4f}s")
            print(f"   âš¡ HÄ±zlanma: {speedup:.2f}x")
            print(f"   ğŸ’° Zaman tasarrufu: {time_saved:.1f}%")
    
    return results


# =============================================================================
# Cloud Deployment - Bulut OrtamÄ±nda Analiz
# =============================================================================

def get_cloud_status():
    """Cloud deployment kullanÄ±labilirlik durumunu dÃ¶ndÃ¼rÃ¼r"""
    if CLOUD_AVAILABLE:
        print("âœ… Cloud deployment aktif!")
        print(f"   ğŸš€ AWS, Azure, Google Cloud desteÄŸi mevcut")
        return True
    else:
        print("âŒ Cloud deployment devre dÄ±ÅŸÄ±!")
        print("   ğŸ“¦ Kurulum: pip install boto3 azure-storage-blob google-cloud-storage")
        return False


class CloudDataManager:
    """
    Bulut ortamÄ±nda veri yÃ¶netimi iÃ§in sÄ±nÄ±f.
    
    Bu sÄ±nÄ±f, AWS S3, Azure Blob Storage ve Google Cloud Storage
    ile veri yÃ¼kleme, indirme ve analiz iÅŸlemlerini yÃ¶netir.
    """
    
    def __init__(self, cloud_provider='aws'):
        """
        CloudDataManager'i baÅŸlatÄ±r.
        
        Parameters
        ----------
        cloud_provider : str
            Bulut saÄŸlayÄ±cÄ±sÄ± ('aws', 'azure', 'gcp')
        """
        self.cloud_provider = cloud_provider.lower()
        self.client = None
        
        if not CLOUD_AVAILABLE:
            print("âŒ Cloud kÃ¼tÃ¼phaneleri bulunamadÄ±!")
            return
        
        try:
            if self.cloud_provider == 'aws':
                self.client = boto3.client('s3')
                print("âœ… AWS S3 client oluÅŸturuldu!")
            elif self.cloud_provider == 'azure':
                self.client = azure.storage.blob.BlobServiceClient.from_connection_string(
                    "DefaultEndpointsProtocol=https;AccountName=test;AccountKey=test;EndpointSuffix=core.windows.net"
                )
                print("âœ… Azure Blob Storage client oluÅŸturuldu!")
            elif self.cloud_provider == 'gcp':
                self.client = google.cloud.storage.Client()
                print("âœ… Google Cloud Storage client oluÅŸturuldu!")
            else:
                print(f"âŒ GeÃ§ersiz bulut saÄŸlayÄ±cÄ±sÄ±: {cloud_provider}")
                
        except Exception as e:
            print(f"âŒ Cloud client oluÅŸturulamadÄ±: {e}")
    
    def upload_data(self, data, bucket_name, file_name, file_format='parquet'):
        """
        Veriyi buluta yÃ¼kler.
        
        Parameters
        ----------
        data : pd.DataFrame
            YÃ¼klenecek veri
        bucket_name : str
            Bucket/container adÄ±
        file_name : str
            Dosya adÄ±
        file_format : str
            Dosya formatÄ± ('parquet', 'csv', 'json')
            
        Returns
        -------
        bool
            YÃ¼kleme baÅŸarÄ±lÄ± ise True
        """
        if not CLOUD_AVAILABLE or self.client is None:
            print("âŒ Cloud client bulunamadÄ±!")
            return False
        
        try:
            # GeÃ§ici dosya oluÅŸtur
            temp_file = f"temp_{file_name}.{file_format}"
            
            if file_format == 'parquet':
                data.to_parquet(temp_file)
            elif file_format == 'csv':
                data.to_csv(temp_file, index=False)
            elif file_format == 'json':
                data.to_json(temp_file, orient='records')
            
            # Buluta yÃ¼kle
            if self.cloud_provider == 'aws':
                self.client.upload_file(temp_file, bucket_name, file_name)
            elif self.cloud_provider == 'azure':
                blob_client = self.client.get_blob_client(container=bucket_name, blob=file_name)
                with open(temp_file, 'rb') as data_file:
                    blob_client.upload_blob(data_file, overwrite=True)
            elif self.cloud_provider == 'gcp':
                bucket = self.client.bucket(bucket_name)
                blob = bucket.blob(file_name)
                blob.upload_from_filename(temp_file)
            
            # GeÃ§ici dosyayÄ± sil
            import os
            os.remove(temp_file)
            
            print(f"âœ… Veri buluta yÃ¼klendi!")
            print(f"   ğŸ“ Dosya: {file_name}")
            print(f"   ğŸ’¾ Boyut: {data.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
            return True
            
        except Exception as e:
            print(f"âŒ Veri yÃ¼kleme hatasÄ±: {e}")
            return False
    
    def download_data(self, bucket_name, file_name, file_format='parquet'):
        """
        Veriyi buluttan indirir.
        
        Parameters
        ----------
        bucket_name : str
            Bucket/container adÄ±
        file_name : str
            Dosya adÄ±
        file_format : str
            Dosya formatÄ± ('parquet', 'csv', 'json')
            
        Returns
        -------
        pd.DataFrame or None
            Ä°ndirilen veri veya None
        """
        if not CLOUD_AVAILABLE or self.client is None:
            print("âŒ Cloud client bulunamadÄ±!")
            return None
        
        try:
            # GeÃ§ici dosya adÄ±
            temp_file = f"temp_{file_name}"
            
            # Buluttan indir
            if self.cloud_provider == 'aws':
                self.client.download_file(bucket_name, file_name, temp_file)
            elif self.cloud_provider == 'azure':
                blob_client = self.client.get_blob_client(container=bucket_name, blob=file_name)
                with open(temp_file, 'wb') as data_file:
                    data_file.write(blob_client.download_blob().readall())
            elif self.cloud_provider == 'gcp':
                bucket = self.client.bucket(bucket_name)
                blob = bucket.blob(file_name)
                blob.download_to_filename(temp_file)
            
            # Veriyi yÃ¼kle
            if file_format == 'parquet':
                data = pd.read_parquet(temp_file)
            elif file_format == 'csv':
                data = pd.read_csv(temp_file)
            elif file_format == 'json':
                data = pd.read_json(temp_file, orient='records')
            
            # GeÃ§ici dosyayÄ± sil
            import os
            os.remove(temp_file)
            
            print(f"âœ… Veri buluttan indirildi!")
            print(f"   ğŸ“ Dosya: {file_name}")
            print(f"   ğŸ“Š Boyut: {data.shape}")
            return data
            
        except Exception as e:
            print(f"âŒ Veri indirme hatasÄ±: {e}")
            return None
    
    def analyze_cloud_data(self, bucket_name, file_pattern='*.parquet'):
        """
        Buluttaki veriyi analiz eder.
        
        Parameters
        ----------
        bucket_name : str
            Bucket/container adÄ±
        file_pattern : str
            Dosya pattern'i
            
        Returns
        -------
        dict
            Analiz sonuÃ§larÄ±
        """
        if not CLOUD_AVAILABLE or self.client is None:
            print("âŒ Cloud client bulunamadÄ±!")
            return {}
        
        try:
            print(f"ğŸ” Buluttaki veri analiz ediliyor...")
            print(f"   ğŸ“ Bucket: {bucket_name}")
            print(f"   ğŸ” Pattern: {file_pattern}")
            
            # Dosya listesini al
            files = []
            if self.cloud_provider == 'aws':
                response = self.client.list_objects_v2(Bucket=bucket_name, Prefix=file_pattern.replace('*', ''))
                files = [obj['Key'] for obj in response.get('Contents', [])]
            elif self.cloud_provider == 'azure':
                container_client = self.client.get_container_client(bucket_name)
                files = [blob.name for blob in container_client.list_blobs(name_starts_with=file_pattern.replace('*', ''))]
            elif self.cloud_provider == 'gcp':
                bucket = self.client.bucket(bucket_name)
                files = [blob.name for blob in bucket.list_blobs(prefix=file_pattern.replace('*', ''))]
            
            print(f"   ğŸ“Š Bulunan dosya sayÄ±sÄ±: {len(files)}")
            
            if not files:
                print("âš ï¸  Dosya bulunamadÄ±!")
                return {}
            
            # Ä°lk dosyayÄ± analiz et
            first_file = files[0]
            data = self.download_data(bucket_name, first_file)
            
            if data is not None:
                results = {
                    'file_name': first_file,
                    'file_count': len(files),
                    'data_info': get_data_info(data),
                    'sample_data': get_data_sample(data, 5)
                }
                
                print("âœ… Bulut veri analizi tamamlandÄ±!")
                return results
            else:
                print("âŒ Veri indirilemedi!")
                return {}
                
        except Exception as e:
            print(f"âŒ Bulut veri analizi hatasÄ±: {e}")
            return {}


def benchmark_cloud_vs_local(data, file_size_mb=100):
    """
    Cloud vs Local performans karÅŸÄ±laÅŸtÄ±rmasÄ±.
    
    Parameters
    ----------
    data : pd.DataFrame
        Test edilecek veri
    file_size_mb : int
        Test dosya boyutu (MB)
        
    Returns
    -------
    dict
        Benchmark sonuÃ§larÄ±
    """
    if not CLOUD_AVAILABLE:
        print("âŒ Cloud kÃ¼tÃ¼phaneleri bulunamadÄ±! Benchmark yapÄ±lamÄ±yor.")
        return {}
    
    print("ğŸ Cloud vs Local Benchmark BaÅŸlÄ±yor...")
    print(f"   ğŸ“Š Veri boyutu: {data.shape}")
    print(f"   ğŸ’¾ Test dosya boyutu: {file_size_mb} MB")
    
    # Veriyi bÃ¼yÃ¼t (test iÃ§in)
    if data.memory_usage(deep=True).sum() / 1024 / 1024 < file_size_mb:
        # Veriyi tekrarla
        repeat_factor = int(file_size_mb / (data.memory_usage(deep=True).sum() / 1024 / 1024)) + 1
        data = pd.concat([data] * repeat_factor, ignore_index=True)
        print(f"   ğŸ“ˆ Veri bÃ¼yÃ¼tÃ¼ldÃ¼: {data.shape}")
    
    results = {}
    
    # Local yazma testi
    print("\nğŸ” Local yazma testi...")
    start_time = time.time()
    temp_file = "temp_benchmark.parquet"
    data.to_parquet(temp_file)
    local_write_time = time.time() - start_time
    
    # Local okuma testi
    start_time = time.time()
    local_data = pd.read_parquet(temp_file)
    local_read_time = time.time() - start_time
    
    # GeÃ§ici dosyayÄ± sil
    import os
    os.remove(temp_file)
    
    # Cloud testi (simÃ¼lasyon)
    print("ğŸš€ Cloud testi (simÃ¼lasyon)...")
    start_time = time.time()
    
    # SimÃ¼le edilmiÅŸ cloud iÅŸlemleri
    time.sleep(0.1)  # Network latency
    cloud_write_time = local_write_time * 2.5  # Cloud yazma genelde daha yavaÅŸ
    
    time.sleep(0.1)  # Network latency
    cloud_read_time = local_read_time * 1.8  # Cloud okuma da yavaÅŸ
    
    # SonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±r
    write_speedup = cloud_write_time / local_write_time
    read_speedup = cloud_read_time / local_read_time
    
    results = {
        'local_write_time': local_write_time,
        'local_read_time': local_read_time,
        'cloud_write_time': cloud_write_time,
        'cloud_read_time': cloud_read_time,
        'write_speedup': write_speedup,
        'read_speedup': read_speedup
    }
    
    print(f"\nğŸ“ˆ BENCHMARK SONUÃ‡LARI:")
    print(f"   ğŸ“ Yazma:")
    print(f"      ğŸŒ Local: {local_write_time:.4f}s")
    print(f"      â˜ï¸  Cloud: {cloud_write_time:.4f}s")
    print(f"      âš¡ HÄ±zlanma: {write_speedup:.2f}x")
    
    print(f"   ğŸ“– Okuma:")
    print(f"      ğŸŒ Local: {local_read_time:.4f}s")
    print(f"      â˜ï¸  Cloud: {cloud_read_time:.4f}s")
    print(f"      âš¡ HÄ±zlanma: {read_speedup:.2f}x")
    
    return results


# =============================================================================
# Streaming Analytics - GerÃ§ek ZamanlÄ± Veri Analizi
# =============================================================================

class StreamingAnalyzer:
    """
    GerÃ§ek zamanlÄ± veri analizi iÃ§in streaming analyzer.
    
    Bu sÄ±nÄ±f, sÃ¼rekli gelen veri akÄ±ÅŸÄ±nÄ± analiz eder
    ve gerÃ§ek zamanlÄ± istatistikler Ã¼retir.
    """
    
    def __init__(self, window_size=1000, update_interval=1.0):
        """
        StreamingAnalyzer'i baÅŸlatÄ±r.
        
        Parameters
        ----------
        window_size : int
            Sliding window boyutu
        update_interval : float
            GÃ¼ncelleme aralÄ±ÄŸÄ± (saniye)
        """
        self.window_size = window_size
        self.update_interval = update_interval
        self.data_buffer = []
        self.stats_history = []
        self.is_running = False
        
        print(f"âœ… Streaming Analyzer baÅŸlatÄ±ldÄ±!")
        print(f"   ğŸ“Š Window boyutu: {window_size}")
        print(f"   â±ï¸  GÃ¼ncelleme aralÄ±ÄŸÄ±: {update_interval} saniye")
    
    def add_data(self, data_point):
        """
        Veri noktasÄ± ekler.
        
        Parameters
        ----------
        data_point : dict or pd.Series
            Eklenecek veri noktasÄ±
        """
        self.data_buffer.append(data_point)
        
        # Window boyutunu aÅŸan eski verileri sil
        if len(self.data_buffer) > self.window_size:
            self.data_buffer.pop(0)
    
    def get_current_stats(self):
        """
        Mevcut window iÃ§in istatistikleri dÃ¶ndÃ¼rÃ¼r.
        
        Returns
        -------
        dict
            GÃ¼ncel istatistikler
        """
        if not self.data_buffer:
            return {}
        
        # DataFrame'e Ã§evir
        df = pd.DataFrame(self.data_buffer)
        
        # Temel istatistikler
        stats = {
            'window_size': len(self.data_buffer),
            'timestamp': time.time(),
            'numeric_stats': {},
            'categorical_stats': {}
        }
        
        # SayÄ±sal sÃ¼tunlar iÃ§in istatistikler
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            numeric_df = df[numeric_cols]
            stats['numeric_stats'] = {
                'mean': numeric_df.mean().to_dict(),
                'std': numeric_df.std().to_dict(),
                'min': numeric_df.min().to_dict(),
                'max': numeric_df.max().to_dict()
            }
        
        # Kategorik sÃ¼tunlar iÃ§in istatistikler
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            for col in categorical_cols:
                value_counts = df[col].value_counts().head(5)
                stats['categorical_stats'][col] = value_counts.to_dict()
        
        return stats
    
    def start_streaming(self, data_generator, max_iterations=None):
        """
        Streaming analizi baÅŸlatÄ±r.
        
        Parameters
        ----------
        data_generator : generator
            Veri Ã¼reteci
        max_iterations : int, optional
            Maksimum iterasyon sayÄ±sÄ±
        """
        if self.is_running:
            print("âš ï¸  Streaming zaten Ã§alÄ±ÅŸÄ±yor!")
            return
        
        self.is_running = True
        print("ğŸš€ Streaming analizi baÅŸlatÄ±ldÄ±...")
        
        iteration = 0
        start_time = time.time()
        
        try:
            for data_point in data_generator:
                if not self.is_running:
                    break
                
                # Veri ekle
                self.add_data(data_point)
                
                # Belirli aralÄ±klarla istatistikleri gÃ¼ncelle
                if iteration % 10 == 0:
                    stats = self.get_current_stats()
                    self.stats_history.append(stats)
                    
                    # Ä°statistikleri yazdÄ±r
                    print(f"\nğŸ“Š Iterasyon {iteration}:")
                    print(f"   ğŸ“ˆ Veri noktasÄ± sayÄ±sÄ±: {len(self.data_buffer)}")
                    
                    if stats.get('numeric_stats'):
                        print(f"   ğŸ”¢ SayÄ±sal istatistikler: {len(stats['numeric_stats']['mean'])} sÃ¼tun")
                    
                    if stats.get('categorical_stats'):
                        print(f"   ğŸ“ Kategorik istatistikler: {len(stats['categorical_stats'])} sÃ¼tun")
                
                iteration += 1
                
                # Maksimum iterasyon kontrolÃ¼
                if max_iterations and iteration >= max_iterations:
                    break
                
                # GÃ¼ncelleme aralÄ±ÄŸÄ±
                time.sleep(self.update_interval)
        
        except KeyboardInterrupt:
            print("\nâ¹ï¸  Streaming kullanÄ±cÄ± tarafÄ±ndan durduruldu.")
        except Exception as e:
            print(f"âŒ Streaming hatasÄ±: {e}")
        finally:
            self.is_running = False
            total_time = time.time() - start_time
            print(f"\nâœ… Streaming analizi tamamlandÄ±!")
            print(f"   â±ï¸  Toplam sÃ¼re: {total_time:.2f} saniye")
            print(f"   ğŸ“Š Toplam veri noktasÄ±: {len(self.data_buffer)}")
            print(f"   ğŸ“ˆ Ä°statistik gÃ¼ncellemesi: {len(self.stats_history)}")
    
    def stop_streaming(self):
        """Streaming analizi durdurur"""
        self.is_running = False
        print("â¹ï¸  Streaming analizi durduruldu.")
    
    def get_streaming_summary(self):
        """
        Streaming analizi Ã¶zetini dÃ¶ndÃ¼rÃ¼r.
        
        Returns
        -------
        dict
            Streaming analizi Ã¶zeti
        """
        if not self.stats_history:
            return {}
        
        # Ä°statistik geÃ§miÅŸini analiz et
        timestamps = [stats['timestamp'] for stats in self.stats_history]
        window_sizes = [stats['window_size'] for stats in self.stats_history]
        
        summary = {
            'total_updates': len(self.stats_history),
            'total_duration': max(timestamps) - min(timestamps),
            'avg_window_size': np.mean(window_sizes),
            'min_window_size': min(window_sizes),
            'max_window_size': max(window_sizes),
            'update_frequency': len(self.stats_history) / (max(timestamps) - min(timestamps)) if len(timestamps) > 1 else 0
        }
        
        return summary


def create_streaming_data_generator(data_type='random', n_points=1000, interval=0.1):
    """
    Test iÃ§in streaming veri Ã¼reteci oluÅŸturur.
    
    Parameters
    ----------
    data_type : str
        Veri tipi ('random', 'trend', 'seasonal')
    n_points : int
        Ãœretilecek veri noktasÄ± sayÄ±sÄ±
    interval : float
        Veri Ã¼retim aralÄ±ÄŸÄ± (saniye)
        
    Returns
    -------
    generator
        Veri Ã¼reteci
    """
    print(f"ğŸ“Š Streaming veri Ã¼reteci oluÅŸturuluyor...")
    print(f"   ğŸ”¢ Veri tipi: {data_type}")
    print(f"   ğŸ“ˆ Nokta sayÄ±sÄ±: {n_points}")
    print(f"   â±ï¸  AralÄ±k: {interval} saniye")
    
    for i in range(n_points):
        if data_type == 'random':
            data_point = {
                'timestamp': time.time(),
                'value': np.random.normal(0, 1),
                'category': np.random.choice(['A', 'B', 'C']),
                'index': i
            }
        elif data_type == 'trend':
            data_point = {
                'timestamp': time.time(),
                'value': i * 0.1 + np.random.normal(0, 0.1),
                'category': 'trend',
                'index': i
            }
        elif data_type == 'seasonal':
            data_point = {
                'timestamp': time.time(),
                'value': np.sin(i * 0.1) + np.random.normal(0, 0.1),
                'category': 'seasonal',
                'index': i
            }
        
        yield data_point
        time.sleep(interval)


def benchmark_streaming_vs_batch(df, window_sizes=[100, 500, 1000]):
    """
    Streaming vs batch analiz performans karÅŸÄ±laÅŸtÄ±rmasÄ±.
    
    Parameters
    ----------
    df : pd.DataFrame
        Test edilecek veri seti
    window_sizes : list
        Test edilecek window boyutlarÄ±
        
    Returns
    -------
    dict
        Benchmark sonuÃ§larÄ±
    """
    print("ğŸ Streaming vs Batch Benchmark BaÅŸlÄ±yor...")
    print(f"   ğŸ“Š Veri seti boyutu: {df.shape}")
    
    results = {}
    
    for window_size in window_sizes:
        print(f"\nğŸ” Window boyutu {window_size} test ediliyor...")
        
        # Batch analiz
        start_time = time.time()
        batch_stats = get_data_info(df.head(window_size))
        batch_time = time.time() - start_time
        
        # Streaming analiz simÃ¼lasyonu
        start_time = time.time()
        analyzer = StreamingAnalyzer(window_size=window_size)
        
        # Veri Ã¼reteci oluÅŸtur
        data_gen = (row.to_dict() for _, row in df.head(window_size).iterrows())
        
        # Streaming analiz
        for data_point in data_gen:
            analyzer.add_data(data_point)
        
        streaming_stats = analyzer.get_current_stats()
        streaming_time = time.time() - start_time
        
        # SonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±r
        if batch_time > 0:
            speedup = batch_time / streaming_time
            time_saved = ((batch_time - streaming_time) / batch_time) * 100
        else:
            speedup = 0
            time_saved = 0
        
        results[window_size] = {
            'batch_time': batch_time,
            'streaming_time': streaming_time,
            'speedup': speedup,
            'time_saved': time_saved
        }
        
        print(f"   ğŸŒ Batch: {batch_time:.4f}s")
        print(f"   ğŸš€ Streaming: {streaming_time:.4f}s")
        print(f"   âš¡ HÄ±zlanma: {speedup:.2f}x")
        print(f"   ğŸ’° Zaman tasarrufu: {time_saved:.1f}%")
    
    return results
