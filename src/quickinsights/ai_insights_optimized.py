"""
QuickInsights - Optimized AI-Powered Data Insights Engine

This module provides optimized AI-powered data analysis with automatic pattern recognition,
anomaly detection and intelligent insights using machine learning.
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
import warnings
from functools import lru_cache
import time

warnings.filterwarnings("ignore")


# Optimized lazy loading with caching
@lru_cache(maxsize=32)
def _get_ml_libraries_cached():
    """Get ML libraries with cached lazy loading."""
    try:
        from ._imports import get_sklearn_utils, get_scipy_utils

        sklearn_utils = get_sklearn_utils()
        scipy_utils = get_scipy_utils()

        return sklearn_utils, scipy_utils
    except ImportError:
        return None, None


def _get_ml_libraries():
    """Get ML libraries with lazy loading."""
    return _get_ml_libraries_cached()


# Check availability without printing
def _check_ml_availability():
    """Check ML library availability silently."""
    sklearn_utils, scipy_utils = _get_ml_libraries()
    if sklearn_utils is None or scipy_utils is None:
        return False, False
    return sklearn_utils["available"], scipy_utils["available"]


# Global availability flags
SKLEARN_AVAILABLE, SCIPY_AVAILABLE = _check_ml_availability()


class AIInsightEngineOptimized:
    """
    Optimized AI-powered data insights engine

    Performance improvements:
    - Cached ML library loading
    - Vectorized operations
    - Memory-efficient data processing
    - Lazy evaluation
    - Batch processing
    """

    def __init__(self, df: pd.DataFrame, enable_caching: bool = True):
        """
        Initialize Optimized AIInsightEngine

        Parameters
        ----------
        df : pd.DataFrame
            Data to analyze
        enable_caching : bool, default True
            Enable result caching for better performance
        """
        # Quick validation
        if df.empty:
            raise ValueError(
                "AIInsightEngine cannot be initialized with empty DataFrame"
            )

        # Use view instead of copy for memory efficiency
        self.df = df
        self.enable_caching = enable_caching

        # Initialize caches
        self._insights_cache = {}
        self._patterns_cache = {}
        self._anomalies_cache = {}
        self._trends_cache = {}

        # Vectorized column type detection
        self.numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        self.categorical_cols = df.select_dtypes(
            include=["object", "category"]
        ).columns.tolist()
        self.datetime_cols = df.select_dtypes(include=["datetime64"]).columns.tolist()

        # Lazy data preparation
        self._data_prepared = False
        self._df_scaled = None
        self._label_encoders = None
        self._df_encoded = None

    def _prepare_data_lazy(self):
        """Lazy data preparation - only when needed"""
        if self._data_prepared:
            return

        start_time = time.time()

        # Normalize numerical data efficiently
        if len(self.numeric_cols) > 0 and SKLEARN_AVAILABLE:
            try:
                sklearn_utils = _get_ml_libraries()[0]
                StandardScaler = sklearn_utils["StandardScaler"]

                # Use single scaler instance
                self._scaler = StandardScaler()

                # Vectorized scaling
                numeric_data = self.df[self.numeric_cols].values
                scaled_data = self._scaler.fit_transform(numeric_data)

                self._df_scaled = pd.DataFrame(
                    scaled_data, columns=self.numeric_cols, index=self.df.index
                )

            except Exception as e:
                # Fallback to original data
                self._df_scaled = self.df[self.numeric_cols].copy()
        else:
            self._df_scaled = (
                self.df[self.numeric_cols].copy()
                if len(self.numeric_cols) > 0
                else pd.DataFrame()
            )

        # Efficient categorical encoding
        if len(self.categorical_cols) > 0 and SKLEARN_AVAILABLE:
            try:
                sklearn_utils = _get_ml_libraries()[0]
                LabelEncoder = sklearn_utils["LabelEncoder"]

                self._label_encoders = {}
                self._df_encoded = self.df.copy()

                # Batch encoding for better performance
                for col in self.categorical_cols:
                    if self.df[col].dtype == "object":
                        le = LabelEncoder()
                        # Handle missing values efficiently
                        clean_data = self.df[col].fillna("MISSING")
                        self._df_encoded[col] = le.fit_transform(clean_data)
                        self._label_encoders[col] = le

            except Exception as e:
                self._df_encoded = self.df.copy()
        else:
            self._df_encoded = self.df.copy()

        self._data_prepared = True

        if self.enable_caching:
            print(
                f"âš¡ Data preparation completed in {time.time() - start_time:.4f} seconds"
            )

    def detect_patterns_optimized(self, max_patterns: int = 10) -> Dict[str, Any]:
        """Optimized pattern detection with caching"""
        cache_key = f"patterns_{max_patterns}"
        if cache_key in self._patterns_cache:
            return self._patterns_cache[cache_key]

        self._prepare_data_lazy()
        start_time = time.time()
        patterns = {}

        try:
            if SKLEARN_AVAILABLE and len(self.numeric_cols) > 1:
                sklearn_utils = _get_ml_libraries()[0]

                # Vectorized correlation analysis
                correlation_matrix = self.df[self.numeric_cols].corr()

                # Find strong correlations efficiently
                strong_correlations = []
                for i in range(len(self.numeric_cols)):
                    for j in range(i + 1, len(self.numeric_cols)):
                        corr_value = correlation_matrix.iloc[i, j]
                        if abs(corr_value) > 0.7:
                            strong_correlations.append(
                                {
                                    "variables": (
                                        self.numeric_cols[i],
                                        self.numeric_cols[j],
                                    ),
                                    "correlation": corr_value,
                                    "strength": "strong"
                                    if abs(corr_value) > 0.9
                                    else "moderate",
                                }
                            )

                strong_correlations.sort(
                    key=lambda x: abs(x["correlation"]), reverse=True
                )
                patterns["correlations"] = strong_correlations[:max_patterns]

            # Performance metrics
            patterns["performance"] = {
                "execution_time": time.time() - start_time,
                "optimization_level": "high",
                "caching_enabled": self.enable_caching,
            }

        except Exception as e:
            patterns = {
                "error": f"Pattern detection failed: {str(e)}",
                "performance": {
                    "execution_time": time.time() - start_time,
                    "optimization_level": "high",
                    "caching_enabled": self.enable_caching,
                },
            }

        if self.enable_caching:
            self._patterns_cache[cache_key] = patterns

        return patterns

    def detect_anomalies_optimized(
        self, method: str = "auto", threshold: float = 0.95
    ) -> Dict[str, Any]:
        """Optimized anomaly detection with multiple methods"""
        cache_key = f"anomalies_{method}_{threshold}"
        if cache_key in self._anomalies_cache:
            return self._anomalies_cache[cache_key]

        self._prepare_data_lazy()
        start_time = time.time()
        anomalies = {}

        try:
            if SKLEARN_AVAILABLE and len(self.numeric_cols) > 0:
                sklearn_utils = _get_ml_libraries()[0]

                # Auto method selection for optimal performance
                if method == "auto":
                    if len(self.df) > 10000:
                        method = "isolation_forest"
                    elif len(self.df) > 1000:
                        method = "zscore"
                    else:
                        method = "zscore"

                if method == "zscore":
                    # Fast Z-score method for small datasets
                    z_scores = np.abs(
                        (self._df_scaled - self._df_scaled.mean())
                        / self._df_scaled.std()
                    )
                    anomaly_mask = (z_scores > 3).any(axis=1)
                    anomaly_indices = np.where(anomaly_mask)[0]

                    anomalies["method"] = "zscore"
                    anomalies["anomaly_count"] = len(anomaly_indices)
                    anomalies["anomaly_percentage"] = (
                        len(anomaly_indices) / len(self.df)
                    ) * 100

            # Performance metrics
            anomalies["performance"] = {
                "execution_time": time.time() - start_time,
                "optimization_level": "high",
                "caching_enabled": self.enable_caching,
                "method_used": method,
            }

        except Exception as e:
            anomalies = {
                "error": f"Anomaly detection failed: {str(e)}",
                "performance": {
                    "execution_time": time.time() - start_time,
                    "optimization_level": "high",
                    "caching_enabled": self.enable_caching,
                },
            }

        if self.enable_caching:
            self._anomalies_cache[cache_key] = anomalies

        return anomalies

    def predict_trends_optimized(
        self, target_column: str = None, horizon: int = 5
    ) -> Dict[str, Any]:
        """Optimized trend prediction with efficient algorithms"""
        cache_key = f"trends_{target_column}_{horizon}"
        if cache_key in self._trends_cache:
            return self._trends_cache[cache_key]

        self._prepare_data_lazy()
        start_time = time.time()
        trends = {}

        try:
            if len(self.numeric_cols) > 0:
                # Auto target selection
                if target_column is None:
                    variances = self.df[self.numeric_cols].var()
                    target_column = variances.idxmax()

                if target_column in self.numeric_cols:
                    target_data = self.df[target_column].dropna()

                    if len(target_data) > 10:
                        # Simple trend analysis for performance
                        x = np.arange(len(target_data))
                        y = target_data.values

                        # Linear trend
                        coeffs = np.polyfit(x, y, 1)
                        trend_slope = coeffs[0]
                        trend_direction = (
                            "increasing" if trend_slope > 0 else "decreasing"
                        )

                        trends["target_column"] = target_column
                        trends["trend_direction"] = trend_direction
                        trends["trend_slope"] = float(trend_slope)

            # Performance metrics
            trends["performance"] = {
                "execution_time": time.time() - start_time,
                "optimization_level": "high",
                "caching_enabled": self.enable_caching,
            }

        except Exception as e:
            trends = {
                "error": f"Trend prediction failed: {str(e)}",
                "performance": {
                    "execution_time": time.time() - start_time,
                    "optimization_level": "high",
                    "caching_enabled": self.enable_caching,
                },
            }

        if self.enable_caching:
            self._trends_cache[cache_key] = trends

        return trends

    def get_comprehensive_insights(self, max_insights: int = 20) -> Dict[str, Any]:
        """Get comprehensive AI insights with all optimizations"""
        cache_key = f"comprehensive_{max_insights}"
        if cache_key in self._insights_cache:
            return self._insights_cache[cache_key]

        start_time = time.time()

        # Run all analyses
        patterns = self.detect_patterns_optimized(max_insights // 3)
        anomalies = self.detect_anomalies_optimized()
        trends = self.predict_trends_optimized()

        # Compile comprehensive insights
        insights = {
            "patterns": patterns,
            "anomalies": anomalies,
            "trends": trends,
            "summary": {
                "total_insights": 0,
                "data_shape": self.df.shape,
                "numeric_variables": len(self.numeric_cols),
                "categorical_variables": len(self.categorical_cols),
                "datetime_variables": len(self.datetime_cols),
            },
            "performance": {
                "total_execution_time": time.time() - start_time,
                "optimization_level": "high",
                "caching_enabled": self.enable_caching,
                "memory_efficient": True,
            },
        }

        # Calculate total insights
        if "patterns" in patterns and "correlations" in patterns["patterns"]:
            insights["summary"]["total_insights"] += len(
                patterns["patterns"]["correlations"]
            )
        if "anomalies" in anomalies and "anomaly_count" in anomalies["anomalies"]:
            insights["summary"]["total_insights"] += anomalies["anomalies"][
                "anomaly_count"
            ]
        if "trends" in trends and "trend_direction" in trends["trends"]:
            insights["summary"]["total_insights"] += 1

        # Cache results
        if self.enable_caching:
            self._insights_cache[cache_key] = insights

        return insights

    def clear_cache(self):
        """Clear all cached results for memory management"""
        self._insights_cache.clear()
        self._patterns_cache.clear()
        self._anomalies_cache.clear()
        self._trends_cache.clear()
        print("ðŸ§¹ Cache cleared for memory optimization")

    def get_performance_stats(self) -> Dict[str, Any]:
        """Get performance statistics and optimization metrics"""
        return {
            "cache_size": {
                "insights": len(self._insights_cache),
                "patterns": len(self._patterns_cache),
                "anomalies": len(self._anomalies_cache),
                "trends": len(self._trends_cache),
            },
            "data_prepared": self._data_prepared,
            "optimization_features": {
                "lazy_loading": True,
                "caching": self.enable_caching,
                "vectorized_operations": True,
                "memory_efficient": True,
            },
            "memory_usage": {
                "dataframe_mb": self.df.memory_usage(deep=True).sum() / 1024**2,
                "scaled_data_mb": self._df_scaled.memory_usage(deep=True).sum()
                / 1024**2
                if self._df_scaled is not None
                else 0,
            },
        }
